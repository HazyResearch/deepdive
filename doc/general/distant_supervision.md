---
layout: default
---

# Distant Supervision

The process of extracting relations from sentences involves a training and testing step. For a more detailed overview of this process, see the [Relation Extraction page](relation_extraction.html). Distant supervision aims to automate the process of generating training examples to be used as input to the training step.

### Overview

Recall our example from the relation extraction section: given the input sentence "Barack Obama was a student at Columbia University" and the target relation Alma_Mater(person, university), we want to extract the relation Alma_Mater(Barack_Obama, Columbia_University). 
The first step of relation extraction is to obtain labeled training examples so that the relation-extraction system can analyze the patterns in those examples. Each example is a mention pair extracted from a sentence, and must be annotated with the relation of which it is a positive instance, or with the fact that it is not a positive instance of any relation. Traditionally these labelings have been manually generated by humans, which is expensive and time-consuming. The goal of distant supervision is to automatically create a large number of positive and negative training examples to alleviate the need for human annotation.

### Drawbacks of Human Annotation

Obtaining human-labeled training examples is difficult to obtain in large amounts. Moreover, since relations are usually labeled on a specific text corpus by domain experts, the resulting relation-extraction systems contain bias toward the input (perform well on test examples that are in the same domain as the training text, and poorly on examples from other domains). To solve this bias would require obtaining training examples from a very large amount of text documents from diverse domains, which is not possible with human annotation. Thus, new techniques such as distant supervision must be investigated.

### Seed Facts

The input to distant supervision is an unlabeled text corpus, a target relation, and a set of known instances of the target relation called seed facts. An example of a seed fact for the target relation `Alma_Mater(person, university)` is `Alma_Mater(Michelle_Obama, Princeton_University)`. These seed facts can come from various sources: for example, they can be generated by the user depending on the application (e.g. known interactions between proteins), or could be obtained from a knowledge base such as Wikipedia (such as the `Alma_Mater(Michelle_Obama, Princeton_University)` example). The purpose of seed facts is to act as the starting points for generating training examples from the text.

### Generating Positive Examples

Given a body of text, a target relation, and a set of seed facts involving pairs of entities participating in the target relation, distant supervision looks through sentences in the text corpus to find mentions involving mentions that link to both entities from each seed fact. If it finds both such mentions in the sentence, then the mention pair is automatically labeled as a positive example for the target relation. For example, given the seed fact `Alma_Mater(Michelle_Obama, Princeton_University)` and the target relation `Alma_Mater(person, university)`, the mention pair ("Michelle Obama", "Princeton University") in the sentence "Michelle Obama went to school at Princeton University" would be recognized as a positive training example for Alma_Mater.

### Generating Negative Examples

We need both positive and negative training examples to create a reliable relation-extraction system. It is often relatively easy to obtain positive examples for distant supervision, but generating negative examples can be somewhat of an art, and there are [many different ways one could go about it](doc/general/generating_negative_examples.html), depending on the data that is available and what assumptions one is willing to make.

### Errors

While distant supervision is able to produce large amounts of training data from only a set of seed facts, the training data it produces contains some incorrect labels. This is expected because not every mention pair will be a positive instance of a single relation. For example, for the above seed fact, the mention pair ("Michelle Obama", "Princeton University") in the sentence "Michelle Obama applied to Princeton University" will be considered a positive example the mentions link to the entities Michelle_Obama and Princeton_University, even though the particular mention pair in that sentence is not indicative of Michelle Obama graduating from Princeton.
The idea that makes distant supervision useful in practice is the hypothesis that broad coverage (text contains lots of mention pairs that correspond to many different target relations) and high redundancy (assuming a seed fact is true, there will be many correct instances involving the 2 entities and fewer incorrect instances) in a large text corpus would compensate for this noise. For example, with a large enough corpus, a distant supervision system may discover that patterns involving mentions in the sentence "Michelle Obama went to school at Princeton University" strongly correlate with seed facts of Attended_University whereas patterns involving mentions in the sentence "Michelle Obama applied to Princeton University" do not qualify as strong indicators. Thus, the quality of distant supervision should improve as larger bodies of text are used.

### Using Distant Supervision

Given a text corpus and a knowledge base containing entities (`Barack_Obama`, `Columbia_University`, etc.), a target relation such as `Alma_Mater(person, university)`, and seed facts such as `Alma_Mater(Barack_Obama, Columbia_University)`, distant supervision can be used to generate positive and negative training examples. For each seed fact and associated target relation we map each of the entity pairs to mentions pairs in sentences of the text (we only consider pairs of mentions in the same sentence), and label those mention pairs as positive examples of the target relation. We then use [various techniques](doc/general/generating_negative_examples.html) to also generate negative examples.

For the target relation `Alma_Mater(Michelle_Obama, Princeton_University)`, both sentences "Michelle Obama applied to Princeton University" and "Michelle Obama went to school at Princeton University" contain mention pairs that link to (`Michelle_Obama`, `Princeton_University`) entities, and so these mention pairs would be labeled as positive examples for the given target relation.

Once the positive and negative training examples are obtained, they are used as a training set for a relation-extraction system. The system would take as input another text corpus containing mention pairs in sentences, identify the entities within those mention pairs, extract features from the sentences containing the mention pairs, and for each relation in the knowledge base, see whether the extracted features indicate that the mention pair is a positive or negative instance of that relation.

### References

- [http://hazy.cs.wisc.edu/hazy/papers/acl2012.pdf](http://hazy.cs.wisc.edu/hazy/papers/acl2012.pdf)
- [http://www.stanford.edu/~jurafsky/mintz.pdf](http://www.stanford.edu/~jurafsky/mintz.pdf)
- [http://www.stanford.edu/class/cs124/lec/rel.pdf](http://www.stanford.edu/class/cs124/lec/rel.pdf)