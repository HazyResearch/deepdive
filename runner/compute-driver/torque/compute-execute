#!/usr/bin/env bash
# torque/compute-execute -- Executes a process remotely using the Torque scheduler
# $ compute-execute input_sql=... command=... output_relation=...
#
# To limit the number of parallel processes, set the DEEPDIVE_NUM_PROCESSES
# environment or the 'deepdive.computers.local.num_processes' in
# computers.conf:
# $ export DEEPDIVE_NUM_PROCESSES=2
# $ compute-execute input_sql=... command=... output_relation=...
##
set -euo pipefail

. commons

: ${DEEPDIVE_PREFIX_TABLE_TEMPORARY:=dd_tmp_} ${DEEPDIVE_PREFIX_TABLE_OLD:=dd_old_}

# load compute configuration
eval "$(jq2sh <<<"$DEEPDIVE_COMPUTER_CONFIG" \
    num_processes='.num_processes' \
    ssh_user='.ssh_user' \
    ssh_host='.ssh_host' \
    remote_deepdive_app_base='.remote_deepdive_app_base' \
    remote_deepdive_cwd_base='.remote_deepdive_cwd_base' \
    poll_period_secs='.poll_period_secs' \
    excludes='.excludes | join("\t")' \
    #
)"
# respect the DEEPDIVE_NUM_PROCESSES environment
num_processes=${DEEPDIVE_NUM_PROCESSES:-${num_processes:-$(
        # detect number of processor cores
        nproc=$(
            # Linux typically has coreutils which includes nproc
            nproc ||
            # OS X
            sysctl -n hw.ncpu ||
            # fall back to 1
            echo 1
        )
        if [[ $nproc -gt 1 ]]; then
            # leave one processor out
            let nproc-=1
        elif [[ $nproc -lt 1 ]]; then
            nproc=1
        fi
        echo $nproc
    )}}

# declare all input arguments
declare -- "$@"

SSH_INFO="${ssh_user}@${ssh_host}"
LOCAL_USER="$(whoami)"

# App directory variables
# APP_ID is used to isolate app folders that are uploaded to the same path
# in the remote node. It is a hash of the local user name and the local absolute 
# DeepDive application path. 
APP_ID="$(hash "$LOCAL_USER + $DEEPDIVE_APP")"
#SESSION_ID="$(date +%Y%m%d/%H%M%S.%N)"
SESSION_ID="$DEEPDIVE_RUN_ID/$DEEPDIVE_CURRENT_PROCESS_NAME"

# Either take the remote DeepDive app path specified by the user, or default to
# remote dir. The app base directory is where DeepDive will create a new directory
# called dd_app, and all apps will be copied to this directory.
remote_deepdive_app_base=${remote_deepdive_app_base:-${remote_deepdive_cwd_base}}
remote_deepdive_app="$remote_deepdive_app_base/dd_app/$APP_ID"

# Current working directory variables.
# TODO: Change "run" to include some session ID to allow multiple processes
# running.
remote_deepdive_cwd="$remote_deepdive_cwd_base/dd_run/$APP_ID/$SESSION_ID"
REMOTE_SH_DIR="$remote_deepdive_cwd/remote.sh"
# TODO: Decoupling input to reduce data transfers. 
REMOTE_IN_DIR="$remote_deepdive_cwd/remote.in"
REMOTE_OUT_DIR="$remote_deepdive_cwd/remote.out"
REMOTE_ERR_DIR="$remote_deepdive_cwd/remote.err"

# show configuration
echo "Executing with the following configuration:"
echo " num_processes=$num_processes"
echo " ssh_user=$ssh_user"
echo " ssh_host=$ssh_host"
echo " remote_deepdive_app_base=$remote_deepdive_app_base"
echo " remote_deepdive_cwd_base=$remote_deepdive_cwd_base"
echo " poll_period_secs=$poll_period_secs"
echo " excludes=$excludes"
echo ""
echo "DEEPDIVE_APP = $DEEPDIVE_APP"
echo "DEEPDIVE_CURRENT_PROCESS_NAME = $DEEPDIVE_CURRENT_PROCESS_NAME"
echo "DEEPDIVE_COMPUTER_CONFIG = $DEEPDIVE_COMPUTER_CONFIG"
echo "DEEPDIVE_COMPUTER_TYPE = $DEEPDIVE_COMPUTER_TYPE"
echo ""
echo "Session information"
echo " application id : $LOCAL_USER + $DEEPDIVE_APP -> $APP_ID"
echo " session id     : $DEEPDIVE_CURRENT_PROCESS_NAME -> $APP_ID/$SESSION_ID"

# ensure all the directories needed exist in the remote node. 
ssh $SSH_INFO bash -c "'
  mkdir -p $(dirname $remote_deepdive_app)
  mkdir -p $remote_deepdive_cwd
'"

# Slash at the end of DEEPDIVE_APP/ is important, so that rsync doesn't create
# another folder inside $remote_deepdive_app
RSYNC_EXCLUDES=""
for exclude in $excludes; do
  # XXX: For some reason, if I enclose $exclude in double quotes and pass it
  # to rsync, rsync will interpret it as '"$exclude"', and fail to find 
  # $exclude.
  RSYNC_EXCLUDES+="--exclude $exclude "
done
echo "Copying deepdive app to remote node."
echo "Excluding $RSYNC_EXCLUDES"
rsync -aH $RSYNC_EXCLUDES --progress $DEEPDIVE_APP/ $SSH_INFO:$remote_deepdive_app

# Prepare submission script
# XXX there are conditional branches below depending on whether input_sql
# and/or output_relation is given, to support four use cases. Depending on
# the cases, need to generate various submission scripts. 
# 1) executing command while streaming data from/to the database
# 2) input-only command which has no output to the database and streams from the database
# 3) output-only command which has no input from the database and streams to the database
# 4) database-independent command which simply runs in parallel

# XXX: Right now, submission script doesn't handle the case when there's no 
# input_sql.
echo "Preparing submission script at remote node"
{
    echo "#PBS -N $DEEPDIVE_CURRENT_PROCESS_NAME"
    if [[ $num_processes -gt 1 ]]; then
        echo "#PBS -l ncpus=1,nodes=$num_processes"
        echo "#PBS -t 0-$((num_processes - 1))"
    fi
    echo "#PBS -o $REMOTE_OUT_DIR"
    echo "#PBS -e $REMOTE_ERR_DIR"
    echo
    echo "export DEEPDIVE_APP=\"$remote_deepdive_app\""
    if [[ $num_processes -gt 1 ]]; then
        echo 'INPUT_ID=$(printf "%04d" $PBS_ARRAYID)'
        echo '"$SHELL"'" -c \"$command\" < \"$REMOTE_IN_DIR."'$INPUT_ID'"\""
    else
        echo '"$SHELL"'" -c \"$command\" < \"$REMOTE_IN_DIR\""
    fi
} | 
ssh $SSH_INFO "cat > $REMOTE_SH_DIR"

# Prepare input data
if [[ -n $input_sql ]]; then
    deepdive-sql eval "$input_sql" format="$DEEPDIVE_LOAD_FORMAT" |
    ssh $SSH_INFO "cat > $REMOTE_IN_DIR"

    # Only perform splitting if num_processes is more than 1
    if [[ $num_processes -gt 1 ]]; then
        { 
            echo "REMOTE_IN_DIR=$REMOTE_IN_DIR"
            echo "num_processes=$num_processes"
            echo 'total_lines=$(wc -l <${REMOTE_IN_DIR})'
            echo 'split -d -l $((total_lines / num_processes + 1)) -a 4 $REMOTE_IN_DIR $REMOTE_IN_DIR.'
        } | ssh $SSH_INFO
    fi
fi


# Submit job to remote node. 
compute_submit() {
  # XXX: Technically, this is all we need under the assumption that the user
  # can do passwordless SSH. These are the correct commands, but need
  # workaround for Macs due to Kerberos issue.
  #ssh $SSH_INFO "qsub -V $REMOTE_SH_DIR"
  
  # XXX: Workaround for Mac OS X users which can't forward Kerberos tickets. 
  ssh $SSH_INFO "env KRB5CCNAME=FILE:$KRB_DIR qsub -V $REMOTE_SH_DIR" 
}

# Ensure Kerberos authentication is succesful for user. 
KRB_DIR="/tmp/krb5cc_${ssh_user}_deepdive"
if ! ssh $SSH_INFO "klist -s -c $KRB_DIR > /dev/null"; then
  echo "Kerberos credentials either expired or not found in remote node."
  echo "Please reenter password."
  ssh -t $SSH_INFO "kinit -r 30d -c $KRB_DIR"
else 
  echo "Kerberos credentials found at $SSH_INFO:$KRB_DIR"
fi

# Submit job. 
# This is a hack that supposedly prints out the output of ssh asking for the 
# password. But the output seems to not be flushed to stdout. 
JOB_ID="$(compute_submit | grep $ssh_host | grep -o ^[0-9]*)"
if [[ ! "$JOB_ID" -gt 0 ]]; then
  echo "Job submission failed with the following error:"
  printf "$JOB_ID"
  error
fi
echo "Job submitted with job id $JOB_ID."

ssh_with_env() {
  ssh $SSH_INFO \
    `env | sed 's/\([^=]*\)=\(.*\)/\1="\2"/' | tr '\n' ' '` \
    "$1"
}

# Poll status
echo "Waiting for job id $JOB_ID to complete"
STATUS=incomplete
#while [[ "$(compute_status)" == "Q" ]] && ssh -q $SSH_INFO "[[ ! -f $REMOTE_OUT_DIR ]]"; do
while [[ "$STATUS" == incomplete ]]; do
  echo "Waiting for $poll_period_secs seconds..."
  sleep $poll_period_secs

  # Efficient way to compute the status as it lifts the burden of checking each
  # job in the submissio node. 
  export JOB_ID 
  export NUM_PROCESSES=$num_processes
  export REMOTE_OUT_DIR
  JOB_STATUSES=$(ssh_with_env JOB_ID NUM_PROCESSES REMOTE_OUT_DIR -- "deepdive compute remote-status")
  
  # Funny way to count number of jobs. 
  NUM_COMPLETED=$(grep -o "C" <<< "$JOB_STATUSES" | grep -c "C")
  NUM_INCOMPLETE=$(grep -o "I" <<< "$JOB_STATUSES" | grep -c "I")
  NUM_FAILED=$(grep -o "F" <<< "$JOB_STATUSES" | grep -c "F")

  if [[ $NUM_INCOMPLETE -eq 0 ]]; then
    if [[ $NUM_FAILED -gt 0 ]]; then
      STATUS=failed
    else
      STATUS=complete
    fi
  fi
done

if [[ "$STATUS" == failed ]]; then
    echo "Job failed!"
    echo "Please inspect $ssh_host:$REMOTE_ERR_DIR for more information"
    echo "----------------------------------------------------------------"
    ssh $SSH_INFO "tail +1 $REMOTE_ERR_DIR*" 
    echo "----------------------------------------------------------------"

    if [[ $num_processes -gt 1 ]]; then
        echo "--- Multi-job report ---"
        echo "Completed jobs: $NUM_COMPLETED"
        echo "Failed jobs: $NUM_FAILED"
        echo "Incomplete jobs: $NUM_INCOMPLETE"
    fi
    error
fi

# prepare a temporary output table when output_relation is given
if [[ -n $output_relation ]]; then
    # some derived values
    output_relation_tmp="${DEEPDIVE_PREFIX_TABLE_TEMPORARY}${output_relation}"

    # show configuration
    echo " output_relation_tmp=$output_relation_tmp"
    echo

    # use an empty temporary table as a sink instead of TRUNCATE'ing the output_relation
    deepdive-create table-if-not-exists "$output_relation"
    db-create-table-like "$output_relation_tmp" "$output_relation"

    # Stream output from remote submission node directly to stdin, and to 
    # deepdive load
    if [[ $num_processes -gt 1 ]]; then
        extract="ssh $SSH_INFO \"cat $REMOTE_OUT_DIR-*\""
    else
        echo "Single process!!"
        extract="ssh $SSH_INFO \"cat $REMOTE_OUT_DIR\""
    fi
    echo $extract
    eval "$extract" > /dev/stdout |
    
    # use mkmimo again to merge outputs of multiple processes into a single stream
    #mkmimo process-*.output \> /dev/stdout |

    # load the output data to the temporary table in the database
    # XXX hiding default progress bar from deepdive-load
    # TODO abbreviate this env into a show_progress option, e.g., recursive=false
    show_progress input_to "$DEEPDIVE_CURRENT_PROCESS_NAME output" -- \
    env DEEPDIVE_PROGRESS_FD=2 \
    deepdive-load "$output_relation_tmp" /dev/stdin

    # rename the new temporary table
    # TODO maybe use PostgreSQL's schema support here?
    echo "Replacing $output_relation with $output_relation_tmp"
    output_relation_old="${DEEPDIVE_PREFIX_TABLE_OLD}${output_relation}"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    deepdive-sql "ALTER TABLE ${output_relation}     RENAME TO ${output_relation_old};"
    deepdive-sql "ALTER TABLE ${output_relation_tmp} RENAME TO ${output_relation};"
    deepdive-sql "DROP TABLE IF EXISTS ${output_relation_old};" || true
    # and analyze the table to speed up future queries
    db-analyze "${output_relation}"
fi
