#!/usr/bin/env jq
# compile-config-2.02-learning_inference -- Adds processes for performing inference with the grounded factor graph
##

include "constants";
include "sql";

# skip adding learning/inference processes unless one for grounding is there
if .deepdive_.execution.processes | has("process/grounding/combine_factorgraph") | not then . else

.deepdive_ as $deepdive
| .deepdive_.execution.processes += {

    # learning weights and doing inference (since we had to load the graph anyway)
    "process/model/learning": {
        dependencies_: ["model/factorgraph"],
        output_: ["model/weights", "data/model/weights"],
        style: "cmd_extractor",
        cmd: "
        export DEEPDIVE_LOAD_FORMAT=tsv
        : ${DEEPDIVE_NUM_PROCESSES:=$(nproc --ignore=1)}
        : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        flatten() { find -L \"$@\" -type f -size +0 -exec pbzip2 -c -d -k {} +; }

        for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do
            results_dir=\"$DEEPDIVE_RESULTS_DIR\"/$pid
            mkdir -p \"$results_dir\"
            fg_dir=\"$DEEPDIVE_FACTORGRAPH_DIR\"/$pid
            [[ -d \"$fg_dir\" ]] || error \"$fg_dir: No factorgraph found\"
            cd \"$fg_dir\"

            # run inference engine for learning and inference

            # decide which set of weights to start learning from
            if [[ $pid -eq 0 ]]; then
                # start from cold weights for the first shard
                weights_hot=weights
            else
                # use hot weights learned from previous shard
                weights_hot=../$(($pid - 1))/weights.learned
            fi
            [[ $weights_hot -ef weights.to_learn ]] || ln -sfn $weights_hot weights.to_learn
            weights=weights.to_learn

            # if there are multiple shards, we suppress inference (-i 0)

            \(if $deepdive.sampler.sampler_cmd == "numbskull" then "
                mkdir -p ./flat
                head -n 1 ./meta        >./flat/graph.meta
                # TODO: save disk space
                flatten ./variables/    >./flat/graph.variables
                flatten ./factors/      >./flat/graph.factors
                flatten ./$weights/     >./flat/graph.weights
                flatten ./domains/      >./flat/graph.domains
                numbskull ./flat \\
                    --output_dir \"$results_dir\" \\
                    --threads $DEEPDIVE_NUM_PROCESSES \\
                    \($deepdive.sampler.sampler_args // "") \\
                    \(if $deepdive.sampler.partitions > 1 then "-i 0" else "" end) \\
                    #
            " else "
                \($deepdive.sampler.sampler_cmd) gibbs      \\
                    --variables <(flatten ./variables/)     \\
                    --domains   <(flatten ./domains/  )     \\
                    --factors   <(flatten ./factors/  )     \\
                    --weights   <(flatten ./$weights/ )     \\
                    --fg_meta   ./meta                      \\
                    --outputFile \"$results_dir\"           \\
                    --n_threads $DEEPDIVE_NUM_PROCESSES     \\
                    \($deepdive.sampler.sampler_args // "") \\
                    \(if $deepdive.sampler.partitions > 1 then "-i 0" else "" end) \\
                    #
            " end)

            # TODO maybe this database interaction for the weights after
            # learning each shard is unnecessary if we have the sampler checkpoint
            # them directly, and we could also keep max two copies

            # load weights to database
            deepdive create table \(deepdiveInferenceResultWeightsTable | @sh) \\
                wid:BIGINT:'PRIMARY KEY' \\
                weight:'DOUBLE PRECISION' \\
                #
            cat \"$results_dir\"/inference_result.out.weights.text |
            tr \(" "|@sh) \("\\t"|@sh) |
            deepdive load \(deepdiveInferenceResultWeightsTable | @sh) /dev/stdin
            deepdive db analyze \(deepdiveInferenceResultWeightsTable | @sh)
            mv -f \"$results_dir\"/inference_result.out.weights.text \"$results_dir\"/inference_result.out.weights.text.learned

            \(if $deepdive.sampler.partitions > 1 then "
                # dump weights back out for next-shard learning or inference step
                mkdir -p weights.learned
                deepdive compute execute \\
                    input_sql=\(
                        { SELECT:
                            [ { table: "w", column: "wid" }
                            , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                            , { expr: "COALESCE(r.weight, w.initvalue, 0)" }
                            ]
                        , FROM: [ { alias: "w", table: deepdiveGlobalWeightsTable } ]
                        , JOIN: { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultWeightsTable }
                                , ON: { eq: [ { table: "r", column: "wid" }
                                            , { table: "w", column: "wid" }
                                            ] }
                                }
                        } | asSql | asPrettySqlArg) \\
                    command=\("
                        sampler-dw text2bin weight /dev/stdin /dev/stdout /dev/null |
                        pbzip2 >weights.learned/weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2
                    " | @sh) \\
                    output_relation=

                # expose the last shard's weights for the inference phase
                [[ $pid -lt \($deepdive.sampler.partitions - 1) ]] ||
                    ln -sfn $pid/weights.learned ../learned_weights

            " else "" end)

        done

        deepdive create view \(deepdiveInferenceResultWeightsMappingView | @sh) as \(
            { SELECT:
                [ { expr: "\"w\".*" }
                , { table: "r", column: "weight" }
                ]
            , FROM: { alias: "w", table: deepdiveGlobalWeightsTable }
            , JOIN: { INNER: { alias: "r", table: deepdiveInferenceResultWeightsTable }
                    , ON: { eq: [ { table: "r", column: "wid" }
                                , { table: "w", column: "wid" }
                                ] } }
            , ORDER_BY: { expr: "ABS(\"r\".\"weight\")", order: "DESC" }
            } | asSql | asPrettySqlArg)
        "
    },

    # performing inference
    "process/model/inference": {
        dependencies_: ["model/factorgraph", "model/weights"],
        output_: ["model/probabilities"],
        style: "cmd_extractor",
        cmd: "
        : ${DEEPDIVE_NUM_PROCESSES:=$(nproc --ignore=1)}
        : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        flatten() { find -L \"$@\" -type f -size +0 -exec pbzip2 -c -d -k {} +; }

        # need to run inference only if learning step was deferred because there are multiple shards
        \(if $deepdive.sampler.partitions > 1 then "
            for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do
                results_dir=\"$DEEPDIVE_RESULTS_DIR\"/$pid
                mkdir -p \"$results_dir\"
                fg_dir=\"$DEEPDIVE_FACTORGRAPH_DIR\"/$pid
                [[ -d \"$fg_dir\" ]] || error \"$fg_dir: No factorgraph found\"
                cd \"$fg_dir\"

                # run sampler for inference with given weights without learning
                ln -sfn ../learned_weights weights.to_infer
                weights=weights.to_infer

                \(if $deepdive.sampler.sampler_cmd == "numbskull" then "
                    mkdir -p ./flat
                    if [ ./meta -nt ./flat/graph.meta ]; then
                        head -n 1 ./meta     >./flat/graph.meta
                        flatten ./variables/ >./flat/graph.variables
                        flatten ./factors/   >./flat/graph.factors
                        flatten ./domains/   >./flat/graph.domains
                    fi
                    flatten ./$weights/ >./flat/graph.weights
                    numbskull ./flat \\
                        --output_dir \"$results_dir\" \\
                        --threads $DEEPDIVE_NUM_PROCESSES \\
                        \($deepdive.sampler.sampler_args // "") \\
                        -l 0 \\
                        #
                " else "
                    \($deepdive.sampler.sampler_cmd) gibbs      \\
                        --variables <(flatten ./variables/)     \\
                        --domains   <(flatten ./domains/  )     \\
                        --factors   <(flatten ./factors/  )     \\
                        --weights   <(flatten ./$weights/ )     \\
                        --fg_meta   ./meta                      \\
                        --outputFile \"$results_dir\"           \\
                        --n_threads $DEEPDIVE_NUM_PROCESSES     \\
                        \($deepdive.sampler.sampler_args // "") \\
                        -l 0 \\
                        #
                " end)

            done

        " else "
            echo Skipping process/model/inference because it was inlined in process/model/learning
        " end)
        "
    },

    "process/model/load_probabilities": {
        dependencies_: ["model/probabilities"],
        output_: ["data/model/probabilities"],
        style: "cmd_extractor",
        cmd: "
            : ${DEEPDIVE_RESULTS_DIR:=\"$DEEPDIVE_APP\"/run/model/results}

            # load weights to database
            deepdive create table \(deepdiveInferenceResultVariablesTable | @sh) \\
                vid:BIGINT \\
                cid:BIGINT \\
                prb:'DOUBLE PRECISION' \\
                #

            # restore shard ID to the vids
            for pid in $(seq 0 \($deepdive.sampler.partitions - 1)); do
                cd $DEEPDIVE_RESULTS_DIR/$pid

                AWK_CMD='{printf \"%s\\t%s\\t%s\\n\", SHARD_BASE + $1, $2, $3}'

                cat inference_result.out.text |
                awk -v SHARD_BASE=$(($pid << 48)) \"$AWK_CMD\" |
                DEEPDIVE_LOAD_FORMAT=tsv \\
                deepdive load \(deepdiveInferenceResultVariablesTable | @sh) /dev/stdin
            done

            deepdive db analyze \(deepdiveInferenceResultVariablesTable | @sh)

            # create a view for each app schema variable
            \([ $deepdive.schema.variables_[] | "
                deepdive create view \("\(.variablesTable)_inference" | @sh) as \(
                { SELECT:
                    [ { expr: "\"v\".*" }
                    , { alias: deepdiveVariableExpectationColumn,   table: "r", column: "prb" }
                    , { alias: deepdiveVariableInternalLabelColumn, table: "i", column: deepdiveVariableInternalLabelColumn }
                    , { alias: deepdiveVariableIdColumn,            table: "i", column: deepdiveVariableIdColumn }
                    ]
                , FROM: [ { alias: "v", table: .variablesTable } ]
                , JOIN:
                    # variable ids
                    [ { LEFT_OUTER: { alias: "i", table: .variablesIdsTable }
                      , ON: { and:  [ .variablesKeyColumns[]
                                    | { eq: [ { table: "i", column: . }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    , if .variableType == "boolean" then
                    # inference results
                      { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultVariablesTable }
                      , ON: { eq:   [ { table: "r", column: "vid" }
                                    , { table: "i", column: deepdiveVariableIdColumn }
                                    ] } }
                    else
                    # category ids are necessary to find the inference result corresponding to the variable
                      { LEFT_OUTER: { alias: "c", table: .variablesCategoriesTable }
                      , ON: { and:  [ .variablesCategoryColumns[]
                                    | { eq: [ { table: "c", column: "_\(.)" }
                                            , { table: "v", column: . }
                                            ] }
                                    ] } }
                    # inference results
                    , { LEFT_OUTER: { alias: "r", table: deepdiveInferenceResultVariablesTable }
                      , ON: { and:  [ { eq: [ { table: "r", column: "vid" }
                                            , { table: "i", column: deepdiveVariableIdColumn }
                                            ] }
                                    , { eq: [ { table: "r", column: "cid" }
                                            , { table: "c", column: "cid" }
                                            ] }
                                    ] } }
                    end
                    ]
                , ORDER_BY:
                    { expr: { table: "r", column: "prb" }
                    , order: "DESC"
                    }
                } | asSql | @sh)"
            ] | join("\n"))
        "
    }

}

end
