#!/usr/bin/env jq
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

include "constants";
include "util";
include "sql";

# skip adding grounding processes unless there are variables and factors defined
if (.deepdive_.schema.variables_  | length) == 0
or (.deepdive_.inference.factors_ | length) == 0
then . else

def factorWeightDescriptionSqlExpr:
    [ ("\(.factorName)-" | asSqlLiteral)
    , (.weight_.params[] |
        "CASE WHEN \(asSqlIdent) IS NULL THEN ''
              ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
          END"
      )
    ] | join(" ||\("-" | asSqlLiteral)|| ")
    ;

# generate a SQL CTE for redefining user's variable table names to have id columns with joins
# necessary for .deepdive.inference.factors.*.input_query as well as .deepdive.calibration.holdout_query
def withRedefinitionsWithJoinsForIdColumns:
    if length > 0 then "WITH \(
        [ .[]
        | "\(.variablesTable | asSqlIdent) AS (\n\(
            { SELECT:
                [ ( .variablesKeyColumns[], deepdiveVariableIdColumn
                | { alias: ., table: "i", column: . } )
                # XXX add these back on demand for the cases where inference rule uses them
                #, ( .variablesCategoryColumns[]
                #| { alias: ., table: "v", column: . } )
                ]
            , FROM: { alias: "i", table: .variablesIdsTable }
            #, JOIN:
            #    [ { LEFT_OUTER: { alias: "v", table: .variablesTable  }
            #      , ON: { and: [ .variablesKeyColumns[]
            #                   | { eq: [ { table: "v", column: . }
            #                           , { table: "i", column: . } ]
            #                     } ] }
            #      } ]
            } | asSql)\n)"
        ] | join("\n   , "))" | "\n\(.)\n----\n"
    else "" end
    ;

.deepdive_ as $deepdive

###############################################################################

## variable/*/materialize
# Each internal variable table holding distinct variables should be
# materialized first for correct id assignment, etc.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/materialize": {
        dependencies_: [
            "process/grounding/from_grounding",
            "data/\(.variablesTable)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        \(if .variableType == "categorical" then "
        # generate a table holding all distinct categories for categorical variables
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesCategoriesTable | @sh) as \(
        { SELECT:
            [ (.variablesCategoryColumns[]
            | { alias: "_\(.)", table: "v", column: . })
            , { alias: "count", expr: "COUNT(1)"             }
            , { alias: "cid"  , expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , GROUP_BY:
            [ .variablesCategoryColumns[]
            | {                 table: "v", column: . } ]
        } | asSql | asPrettySqlArg)
        # assign unique id to every distinct category
        deepdive db assign_sequential_id \(.variablesCategoriesTable | @sh) cid 0
        # TODO simply rely on an env var to determine if this is a dense categorical variable and keep a marker somewhere so the rest of the grounding can determine which approach to use
        # TODO look at the count distribution to determine whether this is actually a dense categorical variable
        " else "" end)

        # generate a table holding all distinct variables identified by @key columns
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesIdsTable | @sh) as \(
        { SELECT:
            [ (.variablesKeyColumns[]
            | { alias: ., table: "v", column: . }) # XXX we don't prefix user's columns (with _ like others) since they directly use this table and it can cause confusion
            , { alias: deepdiveVariableInternalLabelColumn, expr:
                # boolean variables just take one label, so aggregate with AND
                ( if .variableType == "boolean" then "EVERY(\"v\".\(.variablesLabelColumn | asSqlIdent))"
                # categorical variables should point to the category id that has a true label column
                else "MIN(CASE WHEN \"v\".\(.variablesLabelColumn | asSqlIdent) THEN \"c\".\"cid\" ELSE NULL END)"
                end) }
            , { alias: deepdiveVariableInternalFrequencyColumn, expr: "COUNT(1)" }
            , { alias: deepdiveVariableIdColumn, expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , JOIN:
            ( if .variableType == "boolean" then null else
                # categorical variables need a join with the categories table
                # to find the correct internal label, etc.
                [ { LEFT_OUTER: { alias: "c", table: .variablesCategoriesTable }
                  , ON: { and: [ .variablesCategoryColumns[]
                               | { eq: [ { table: "c", column: "_\(.)" }
                                       , { table: "v", column: . } ]
                                 } ] }
                  } ]
            end)
        , GROUP_BY: [ .variablesKeyColumns[] | { column: . } ]
        } | asSql | asPrettySqlArg)
        "
    }
})

## variable_id_partition
# Grounding begins by counting the variables to partition a range of
# non-negative integers for assigning the variable ids.
| .deepdive_.execution.processes += {
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all distinct variables to be first identified
            $deepdive.schema.variables_[] | "process/grounding/variable/\(.variableName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables_ | map(.variablesIdsTable | @sh) | join(" ")) | {
            # variable names
            set -- \($deepdive.schema.variables_ | map(.variableName | @sh) | join(" "))
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varName=$1; shift
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${varName}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    }
}


## variable/*/assign_id
# Each variable table then gets the range of integers assigned to the id column
# of every row.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/assign_id": {
        dependencies_: [
            "process/grounding/variable_id_partition"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        baseId=$(cat id_begin)

        # assign id to all distinct variables according to the paritition
        deepdive db assign_sequential_id \(.variablesIdsTable | @sh) \(deepdiveVariableIdColumn | @sh) $baseId
        "
    }
})

## variable_holdout
# Variables to holdout are recorded by executing either a user-defined
# (app-wide) holdout query, or by taking a random sample of a user-defined
# fraction.
# TODO easier way to do holdout per variable?
| .deepdive_.execution.processes += {
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        deepdive create table \(deepdiveGlobalHoldoutTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        deepdive create table \(deepdiveGlobalObservationTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        \([ if $deepdive.calibration.holdout_query then
            # run user holdout query if configured
            "\($deepdive.schema.variables_
                | withRedefinitionsWithJoinsForIdColumns
            )\($deepdive.calibration.holdout_query)"
          else
            # otherwise, randomly select from evidence variables of each variable table
            $deepdive.schema.variables_[] | "
                INSERT INTO \(deepdiveGlobalHoldoutTable | asSqlIdent) \(
                { SELECT: [ { column: deepdiveVariableIdColumn } ]
                , FROM: [ { table: .variablesIdsTable } ]
                , WHERE:
                    [ { isntNull: { column: deepdiveVariableInternalLabelColumn } }
                    , { lt: [ { expr: "RANDOM()" }
                            , { expr: $deepdive.calibration.holdout_fraction }
                            ]
                      }
                    ]
                } | asSql);
            "
          end
        , if $deepdive.calibration.observation_query then
            # run user holdout query if configured
            "\($deepdive.schema.variables_
                | withRedefinitionsWithJoinsForIdColumns
            )\($deepdive.calibration.holdout_query)"
          else empty
          end
        ] | map("deepdive sql \(asPrettySqlArg)") | join("\n"))
        "
    }
}

## variable/*/dump
# Then each variable table is dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/dump": {
        dependencies_: [
            "process/grounding/variable_holdout"
          # XXX below can be omitted for now
          #, "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'variables.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # dump the variables, joining the holdout query to determine the type of each variable
        deepdive compute execute \\
            input_sql=\(
            { SELECT:
                [ { column: "vid" }
                , { column: "variable_role" }
                , { alias: "init_value", expr:
                    "CASE WHEN variable_role = 0 THEN 0
                          ELSE (\(
                            if   .variableType == "boolean"     then "CASE WHEN label THEN 1 ELSE 0 END" # XXX a portable way to turn boolean to integers in SQL, CAST(label AS INT) does not work for MySQL
                            elif .variableType == "categorical" then "label"
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end
                            )) + 0.0
                      END" }
                , { column: "variable_type" }
                , { column: "cardinality" }
                ]
            , FROM: { alias: "variables", sql:
                { SELECT:
                    [ { alias: "vid", column: deepdiveVariableIdColumn }
                    , { alias: "variable_role", expr:
                          "CASE WHEN observation.variable_id IS NOT NULL
                                 AND \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 2
                                WHEN holdout.variable_id IS NOT NULL THEN 0
                                WHEN \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 1
                                ELSE 0
                            END" }
                    , { alias: "label", table: "i", column: deepdiveVariableInternalLabelColumn }
                    , { alias: "variable_type", expr:
                            ( if .variableType == "boolean"     then 0
                            elif .variableType == "categorical" then 1
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    , { alias: "cardinality", expr:
                            ( if .variableType == "boolean"     then 2
                            # TODO dense categorical can use a constant here
                            elif .variableType == "categorical" then "i.count" # TODO count the number of actual distinct values by .variablesCategoryColumns for this row
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    ]
                , FROM: { alias: "i", table: .variablesIdsTable }
                , JOIN:
                    [ { LEFT_OUTER: { alias: "holdout", table: deepdiveGlobalHoldoutTable }
                      , ON: { eq: [ { table: "i", column: deepdiveVariableIdColumn }
                                  , { table: "holdout"  , column: "variable_id" } ] } }
                    , { LEFT_OUTER: { alias: "observation", table: deepdiveGlobalObservationTable }
                      , ON: { eq: [ { table: "i"  , column: deepdiveVariableIdColumn }
                                  , { table: "observation", column: "variable_id" } ] } }
                    , ( if .variableType != "categorical" then empty else
                      { LEFT_OUTER: { alias: "categories" , table: .variablesCategoriesTable }
                      , ON: { eq: [ { table: "i"  , column: deepdiveVariableIdColumn }
                                  , { table: "observation", column: "variable_id" } ] } }
                      end)
                    ]
                } }
            } | asSql | asPrettySqlArg) \\
            command=\("
                format_converter variable /dev/stdin >(pbzip2 >variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        "
    }

})


## variable/*/dump_domains
# Each categorical variable dumps an extra input for the inference engine that holds the domains.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[]
    | select(.variableType == "categorical") | {
    "process/grounding/variable/\(.variableName)/dump_domains": {
        dependencies_: [
            "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'domains.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # TODO dense categorical variables don't need these
        # dump the categorical variable domains, joining the categories table for their ids
        deepdive compute execute \\
            input_sql=\(
            # TODO dense categorical don't need JOIN and no dump since a view that cross products every variable with all categories
            # sparse categorical variables have domains files
            { SELECT:
                [ { alias: "vid",  table: "i", column: deepdiveVariableIdColumn }
                , { alias: "cardinality", expr: "COUNT(c.cid)" }
                , { alias: "cids", expr: "ARRAY_AGG(c.cid)" }
                ]
            , FROM: [ { alias: "v", table: .variablesTable } ]
            , JOIN:
                # variable ids
                [ { LEFT_OUTER: { alias: "i", table: .variablesIdsTable }
                  , ON: { and:  [ .variablesKeyColumns[]
                                | { eq: [ { table: "i", column: . }
                                        , { table: "v", column: . }
                                        ] }
                                ] } }
                # category ids are necessary to find the inference result corresponding to the variable
                , { LEFT_OUTER: { alias: "c", table: .variablesCategoriesTable }
                  , ON: { and:  [ .variablesCategoryColumns[]
                                | { eq: [ { table: "c", column: "_\(.)" }
                                        , { table: "v", column: . }
                                        ] }
                                ] } }
                ]
            , GROUP_BY: [ { table: "i", column: deepdiveVariableIdColumn } ]
            } | asSql | asPrettySqlArg) \\
            command=\("
                exec cat >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.tsv  # TODO @feiranwang remove this line once format_converter is ready
                format_converter domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        "
    }

})


###############################################################################

## factor/*/materialize
# Each inference rule's SQL query is run to materialize the factors and the
# distinct weights used in them.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[]
    | [ .input_[]
      | ltrimstr("data/")
      | $deepdive.schema.variables_byName[.]
      | select(type != "null")
      ] as $schemaVariablesForThisFactor
    | {
    # add a process for grounding factors
    "process/grounding/factor/\(.factorName)/materialize": {
        # materializing each factor requires the dependent variables to have their id assigned
        dependencies_: [
            $schemaVariablesForThisFactor[]
            # the involved variables must have their ids all assigned
            | "process/grounding/variable/\(.variableName)/assign_id"?
        ],
        # other non-variable tables are also necessary
        input_: [ .input_[]
            | select(ltrimstr("data/") | in($deepdive.schema.variables_byName) | not)
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            # materialize factors using user input_query that pulls in assigned ids to involved variables
            deepdive create table \(.factorsTable | @sh) as \(
                # allow .input_query to refer to the assigned variable ids by redefining the variable table names with SQL CTE
                { SELECT:
                    [ ( .function_.variables[] | { table: "F", column: .columnId } )
                    , ( .weight_.params[]      | { table: "F", column: . } )
                    ]
                , DISTINCT: true  # XXX maybe we should assume user is going to be make this true
                , FROM: [ { alias: "F", sql:
                    "\($schemaVariablesForThisFactor
                        | withRedefinitionsWithJoinsForIdColumns
                    )\(.input_query)" } ]
                } | asSql | asPrettySqlArg)

            # find distinct weights for the factors into a separate table
            deepdive create table \(.weightsTable | @sh) as \(
                if .function_.isMultinomial then
                    # TODO dense multinomial won't have to find exact weight combinations
                    # factors over categorical variables
                    { SELECT:
                        # weight parameters
                        [ ( .weight_.params[] | { column: . } )
                        # include category columns for every categorical variable
                        , ( .function_.variables[]
                        | .columnId as $columnIdForThisVar
                        | .schema.variablesCategoryColumns[]
                        | { alias: "\($columnIdForThisVar)_\(.)", table: "v.\($columnIdForThisVar)", column: . } )
                        # weight attributes
                        , { alias: "isfixed"  , expr: .weight_.is_fixed      }
                        , { alias: "initvalue", expr: .weight_.init_value    }
                        , { alias: "wid"      , expr: "CAST(NULL AS BIGINT)" }
                        ]
                    , DISTINCT: true  # XXX this is inevitable
                    # weights are exploded for the factor's variables' category values
                    , FROM: [ { alias: "f", table: .factorsTable } ]
                    # FIXME joins below can go away if DDlog could project them as well in the .input_query
                    # or we could inject those pushed-down projections if it at least compiled into a sql.jq object
                    , JOIN:
                        [ ( .function_.variables[]
                        | .columnId as $columnIdForThisVar
                        | { LEFT_OUTER: { alias: "i.\($columnIdForThisVar)", table: .schema.variablesIdsTable }
                          , ON: { eq: [ { table: "f"                          , column: $columnIdForThisVar }
                                      , { table: "i.\($columnIdForThisVar)", column: deepdiveVariableIdColumn } ] }
                          }
                        , { LEFT_OUTER: { alias: "v.\($columnIdForThisVar)", table: .schema.variablesTable }
                          , ON: [ ( .schema.variablesKeyColumns[]
                                | { eq: [ { table: "v.\($columnIdForThisVar)", column: . }
                                        , { table: "i.\($columnIdForThisVar)", column: . } ] } ) ] }
                        ) ]
                    }
                else
                    # factors over boolean variables
                    { SELECT:
                        # weight parameters
                        [ ( .weight_.params[] | { column: . } )
                        # weight attributes
                        , { alias: "isfixed"  , expr: .weight_.is_fixed      }
                        , { alias: "initvalue", expr: .weight_.init_value    }
                        , { alias: "wid"      , expr: "CAST(NULL AS BIGINT)" }
                        ]
                    , DISTINCT: true  # XXX this is inevitable
                    , FROM:
                        [ select(.weight_.params | length > 0)
                        # when weight is parameterized, find all distinct ones
                        | { alias: "f", table: .factorsTable }
                        ]
                    }
                end | asSql | asPrettySqlArg)
        "
    }
})


## weight_id_partition
# The weight ids must be first partitioned by counting them.
| .deepdive_.execution.processes += {
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 RANGE_STEP=1 \\
        partition_id_range \([ $deepdive.inference.factors_[] | "\(.weightsTable | @sh)\(
            # TODO dense multinomial should use a different step size
            ""
                #if .function_.name != "multinomial" then ""
                #else ":$(( \([ .function_.variables[]
                #             | .schema.variableCardinality
                #             | tostring ] | join("*")) ))"
                    # TODO switch to bc to handle arbitrarily large numbers
        )" ] | join(" ")) | {
            # factor names
            set -- \($deepdive.inference.factors_ | map(.factorName | @sh) | join(" "))
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=$1; shift
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }
        "
    }
}

## global_weight_table
# To view the weights learned by the inference engine later, set up an app-wide table.
| .deepdive_.execution.processes += {
    "process/grounding/global_weight_table": {
        dependencies_: [
            $deepdive.inference.factors_[] |
            if .function_.isMultinomial then
                # TODO this is only applicable to dense multinomial
                "process/grounding/factor/\(.factorName)/assign_weight_id"
            else
                "process/grounding/factor/\(.factorName)/materialize"
            end
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # set up a union view for all weight tables (\(deepdiveGlobalWeightsTable | asSqlIdent))
        deepdive create view \(deepdiveGlobalWeightsTable | @sh) as \(
            [ $deepdive.inference.factors_[] |
                { SELECT:
                    [ { column: "wid" }
                    , { column: "isfixed" }
                    , { column: "initvalue" }
                    , { alias: "description", expr: factorWeightDescriptionSqlExpr
                      }
                    , if .function_.isMultinomial then
                        # serialize category values describing an individual multinomial factor weight
                        { alias: "categories", expr:
                            [ ( .function_.variables[]
                            | .columnId as $columnIdForThisVar
                            | ( .schema.variablesCategoryColumns[]
                            | "\($columnIdForThisVar)_\(.)" | asSqlIdent )
                            ) ] | join(" ||'\t'|| ")
                        }
                    else
                        { alias: "categories", expr: "NULL" }
                    end
                    ]
                , FROM:
                    [ { table: .weightsTable }
                    ]
                } | asSql | "(\(.))"
            ] | join("\nUNION ALL\n") | asPrettySqlArg)
        "
    }
}

## factor/*/assign_weight_id
# Each inference rule gets its weight ids actually assigned.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    "process/grounding/factor/\(.factorName)/assign_weight_id": {
        dependencies_: [
            "process/grounding/weight_id_partition"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            baseId=$(cat weights_id_begin)
            # assign weight ids according to the partition
            deepdive db assign_sequential_id \(.weightsTable | @sh) wid $baseId 1

            # TODO dense multinomial should take product of cardinalities of all variables
            #\( if .function_.name != "multinomial" then ""
            #else
            #inc=1
            #    "# since this is a multinomial factor, each factor will have as many weights as the product of cardinalities of all involved variables
            #    \( [ .function_.variables[] |
            #        # TODO switch to bc to handle arbitrarily large numbers
            #        "let inc*=\(.schema.variableCardinality | tostring)  # cardinality of variable \(.name)"
            #    ] | join("\n") )"
            #end)

            # TODO dense multinomial should create a view that can be used for dumping weights in dense representation
            ## assign weight ids according to the partition
            #deepdive db assign_sequential_id \(.weightsTable | @sh) id $baseId $inc
            ## set up an exploded weights view that cross joins the base weights table with the categories of categorical variables
            #deepdive create view \(.weightsTableForDumping | @sh) as \(
            #    { SELECT:
            #        [ ( .weight_.params[]
            #        | { column: .          , table: "weights" } )
            #        , { column: "isfixed"  , table: "weights" }
            #        , { column: "initvalue", table: "weights" }
            #        # fill in the ids of weights for every combination of categories
            #        , { alias: "id", expr: "\"weights\".\"id\" + (\(
            #                def offsetExpr(i): .[i].schema as $v |
            #                    if i == 0 then "" # except the very first variable
            #                    else # the offset for the previous ones are multiplied by this one's cardinality
            #                        "(\(offsetExpr(i-1)))*\($v.variableCardinality | tostring) + "
            #                    end
            #                    # and current variable's category number is added as the least significant digit
            #                    + "\"c\(i)\".\"category\""
            #                    ;
            #                .function_.variables | offsetExpr(length-1)
            #            ))" }
            #        # generate a category combination identifier
            #        , { alias: "categories", expr: (
            #            .function_.variables | if length > 1 then
            #                [ .[] | "\"c\(.ordinal)\".\"category\"" ] | join(" || ',' || ")
            #            else
            #                "\"c0\".\"category\" || ''"
            #            end)
            #          }
            #        ]
            #    , FROM:
            #        [ { alias: "weights", table: .weightsTable }
            #        # cross join each weight with the categories of all involved categorical variables
            #        , ( .function_.variables[]
            #        | { alias: "c\(.ordinal)", table: .schema.variableCategoriesTable } )
            #        ]
            #    } | asSql | @sh)"
        "
    }
})


## factor/*/dump
# The factors are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'factors.part-*.bin.bz2' \\
                    -o -name 'nfactors.part-*'         \\
                    -o -name   'nedges.part-*'         \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # dump the factors joining the assigned weight ids, converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\(
                if .function_.isMultinomial then
                    # factors over categorical variables
                    # TODO dense multinomial need not join the weight ids, since it exploits the fact that all possible weights are materialized, and each factor can use the domain values to index into the exact one
                    # a sparse multinomial factor should find weights corresponding to its variables' category values
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        , { alias: "num_weights", expr: "COUNT(w.wid)" }
                        , { alias: "weight_ids" , expr: "ARRAY_AGG(w.wid)" }
                        ]
                    , FROM:
                        [ { alias: "w", table: .weightsTable }
                        , { alias: "f", table: .factorsTable }
                        ]
                    , JOIN:
                        # sparse multinomial factors need to know exact category values and weight parameters combination present in the data, XXX hence a lot of join conditions
                        # FIXME joins below can go away if .factorsTable just duplicate these columns all from the "/materialize" process
                        [ ( .function_.variables[]
                        | .columnId as $columnIdForThisVar
                        | { LEFT_OUTER: { alias: "i.\($columnIdForThisVar)", table: .schema.variablesIdsTable }
                          , ON: [ { eq: [ { table: "f"                          , column: $columnIdForThisVar }
                                        , { table: "i.\($columnIdForThisVar)", column: deepdiveVariableIdColumn } ] }
                                ] }
                        , { LEFT_OUTER: { alias: "v.\($columnIdForThisVar)", table: .schema.variablesTable }
                          , ON: [ ( .schema.variablesKeyColumns[]
                                | { eq: [ { table: "v.\($columnIdForThisVar)", column: . }
                                        , { table: "i.\($columnIdForThisVar)", column: . } ] } )
                                ] } )
                        ]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        , ( .function_.variables[]
                        | .columnId as $columnIdForThisVar
                        | ( .schema.variablesCategoryColumns[]
                        | { eq: [ { table: "v.\($columnIdForThisVar)", column: . }
                                , { table: "w", column: "\($columnIdForThisVar)_\(.)" } ] } ) )
                        ]
                    , GROUP_BY:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        ]
                    }
                else
                    # factors over boolean variables
                    { SELECT:
                        # FIXME @feiranwang reorder after fixing formater converter
                        [ { alias: "weight_id", table: "w", column: "wid" }
                        , ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        ]
                        # [ ( .function_.variables[]
                        # | { table: "f", column: .columnId } )
                        # , { alias: "weight_id", table: "w", column: "wid" }
                        # ]
                    , FROM:
                        [ { alias: "w", table: .weightsTable }
                        , { alias: "f", table: .factorsTable }
                        ]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        ]
                    }
                end | asSql | asPrettySqlArg) \\
                command=\("
                    # also record the factor count
                    tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                    format_converter factor /dev/stdin >(pbzip2 >factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2) \(.function_.id
                        ) \(.function_.variables | length
                        ) original \(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ")
                        ) |
                    # and the edge count
                    tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                " | @sh) \\
                output_relation=
        "
    }
})

## factor/*/dump_weights
# The factors and weights are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump_weights": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'weights.part-*.bin.bz2' \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # flag that signals whether to reuse weights or not
            reuseFlag=\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights.reuse

            # dump the weights (except the description column), converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\"$(if [[ -e \"$reuseFlag\" ]]; then
                    echo \(
                    # dump weights with initvalue from previously learned ones
                    { SELECT:
                        [ { table: "w", column: "wid" }
                        , { expr: "CASE WHEN w.isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(reuse.weight, w.initvalue, 0)" }
                        ]
                    , FROM: [ { alias: "w", table: .weightsTable } ]
                    , JOIN: { LEFT_OUTER: { alias: "reuse", table: deepdiveReuseWeightsTable }
                            , ON: { and: [ { eq: [ { table: "reuse", column: "description" }
                                                 , { expr: factorWeightDescriptionSqlExpr }
                                                 ] }
                                         , if .function_.isMultinomial then
                                           { or: [ { isNull: { table: "reuse", column: "categories" } }
                                                   , { eq: [ { table: "reuse", column: "categories" }
                                                           , { table: "w",     column: "categories" }
                                                           ] }
                                                 ] }
                                           else empty end
                                         ] }
                            }
                    } | asSql | asPrettySqlArg)
                else
                    echo \(
                    # dump weights from scratch
                    { SELECT:
                        [ { column: "wid" }
                        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(initvalue, 0)" }
                        ]
                    , FROM: [ { table: .weightsTable } ]
                    } | asSql | asPrettySqlArg)
                fi)\" \\
                command=\("
                    format_converter weight /dev/stdin >(pbzip2 >weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
                " | @sh) \\
                output_relation=
        "
    }
})

###############################################################################

# Finally, put together everything dumped into a layout the inference engine can easily load from
| .deepdive_.execution.processes += {
    "process/grounding/combine_factorgraph": {
        dependencies_: [(
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/dump"
            , (select(.variableType == "categorical")
            | "process/grounding/variable/\(.variableName)/dump_domains")
        ), (
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/dump"
            , "process/grounding/factor/\(.factorName)/dump_weights"
        ), (
            "process/grounding/global_weight_table"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: (
            ([$deepdive.schema.variables_[] | .variableName | @sh] | join(" ")) as $variableNames |
            ([$deepdive.inference.factors_[] | .factorName  | @sh] | join(" ")) as $factorNames   |
        "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        # create a fresh empty directory for the new combined factor graph
        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \($variableNames); do
            mkdir -p {variables,domains}/\"$v\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\" \\
                -name 'variables.part-*.bin.bz2' -exec ln -sfnv -t variables/\"$v\"/ {} + \\
                -o \\
                -name   'domains.part-*.bin.bz2' -exec ln -sfnv -t   domains/\"$v\"/ {} + \\
                #
        done
        for f in \($factorNames); do
            mkdir -p {factors,weights}/\"$f\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\" \\
                -name 'factors.part-*.bin.bz2' -exec ln -sfnv -t factors/\"$f\"/ {} + \\
                -o \\
                -name 'weights.part-*.bin.bz2' -exec ln -sfnv -t weights/\"$f\"/ {} + \\
                #
        done

        # generate the metadata for the inference engine
        {
            # first line with counts of variables and edges in the grounded factor graph
            cd \"$DEEPDIVE_GROUNDING_DIR\"
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            counts=()
            counts+=($(cat factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat variable_count))
            cd factor
            counts+=($(find \($factorNames) -name 'nfactors.part-*' -exec cat {} + | sumup))
            counts+=($(find \($factorNames) -name 'nedges.part-*'   -exec cat {} + | sumup))
            (IFS=,; echo \"${counts[*]}\")
            # second line with file paths
            paths=(\"$DEEPDIVE_FACTORGRAPH_DIR\"/{weights,variables,factors,edges,domains})
            (IFS=,; echo \"${paths[*]}\")
        } >meta
        ")
    }
}

## from_grounding
# A nominal process to make it easy to redo the grounding
# TODO remove this once deepdive-do supports process groups or pipelines
| .deepdive_.execution.processes += {
    "process/grounding/from_grounding": {
        style: "cmd_extractor", cmd: ": no-op"
    }
}

end
