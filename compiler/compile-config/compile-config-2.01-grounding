#!/usr/bin/env jq-f
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

# TODO take these out to a separate file
###############################################################################
def merge(objects): reduce objects as $es ({}; . + $es);

def trimWhitespace: gsub("^\\s+|\\s+$"; ""; "m");

def nullOr(expr): if type == "null" then null else expr end;

###############################################################################
## helper functions for SQL generation
###############################################################################

# a handy way to quote SQL identifiers
# See: http://www.postgresql.org/docs/current/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
def asSqlIdent: "\"\( tostring  # anything can be turned into a SQL identifier if they are inside double quotes
                    | gsub("\""; "\"\"") # even double quote themselves (in fact, PostgreSQL says \0 cannot be included)
                    )\"";

# a handy way to turn things into SQL string literals (as it is hard to write single quote within single quote in this Bash script)
def asSqlLiteral:
    # map null to NULL
    if type == "null" then "NULL"
    # shouldn't turn primitive data types into string literals
    elif type | in({number:1, boolean:1}) then tostring
    # surround strings with single quotes with appropriate escaping
    # and turn any array or object into JSON format
    else "'\(tostring | gsub("'"; "''"))'"
    end;

# a handy way to generate a string from an array with prefix/suffix and delimiters
# where an empty string is desired for empty input
# to simplify the following SQL SELECT query generation
def mapJoinString(prefix; eachItem; delimiter; suffix):
    "\(prefix)\(map(eachItem) | join(delimiter))\(suffix)";
def mapJoinOrEmptyString(prefix; eachItem; delimiter; suffix):
    if type == null then null
    elif length == 0 then ""
    else
        if type == "array" then . else [.] end |
        mapJoinString(prefix; eachItem; delimiter; suffix)
    end;
# some short hands
def mapJoinString(prefix; eachItem; delimiter)        : mapJoinString(prefix; eachItem; delimiter; "");
def mapJoinString(prefix; eachItem)                   : mapJoinString(prefix; eachItem; "");
def mapJoinString(prefix)                             : mapJoinString(prefix; .);
def mapJoinOrEmptyString(prefix; eachItem; delimiter) : mapJoinOrEmptyString(prefix; eachItem; delimiter; "");
def mapJoinOrEmptyString(prefix; eachItem)            : mapJoinOrEmptyString(prefix; eachItem; "");
def mapJoinOrEmptyString(prefix)                      : mapJoinOrEmptyString(prefix; .);

# compile SQL expression: table.column or generic expression
def asSqlExpr:
    if has("expr")                       then .expr | tostring  # TODO support more structured expressions, e.g., binary operators and asSqlCondition
    elif has("table") and has("column") then "\(.table | asSqlIdent).\(.column | asSqlIdent)"
    elif has("column")                   then .column | asSqlIdent
    else error("Neither keys .expr or .column found for SQL expression in \(tostring)")
    end;
# compile `expression AS alias` in the field list of SELECT clauses
def asSqlExprAlias:
    (asSqlExpr) + (.alias | mapJoinOrEmptyString(" AS "; asSqlIdent));
# compile `table alias` mainly for the FROM or JOIN clauses
def asSqlTableAlias(asSql):
    if has("table") then
        (.table | asSqlIdent) +
        (.alias | mapJoinOrEmptyString(" "; asSqlIdent))
    elif has("sql") then
        "(\(.sql | asSql)) " +
        (.alias // error(".alias must be set for subquery \(.sql | tostring)") | asSqlIdent)
    else error("Neither keys .table or .sql found for FROM clause in \(tostring)")
    end;
def asSqlJoinTypeTableAlias(asSql):
    # five types of joins in ANSI SQL standard: https://en.wikipedia.org/wiki/Join_(SQL)
    (.LEFT_OUTER  | mapJoinOrEmptyString("LEFT OUTER JOIN "  ; asSqlTableAlias(asSql))) //
    (.RIGHT_OUTER | mapJoinOrEmptyString("RIGHT OUTER JOIN " ; asSqlTableAlias(asSql))) //
    (.FULL_OUTER  | mapJoinOrEmptyString("FULL OUTER JOIN "  ; asSqlTableAlias(asSql))) //
    (.INNER       | mapJoinOrEmptyString("INNER JOIN "       ; asSqlTableAlias(asSql))) //
    (.CROSS       | mapJoinOrEmptyString("CROSS JOIN "       ; asSqlTableAlias(asSql))) //
    error("Join table must be specified under one of these keys: LEFT_OUTER, RIGHT_OUTER, FULL_OUTER, INNER, CROSS, but found: \(tostring)");
# compile SQL conditional expressions for WHILE, HAVING, and JOIN ON clauses
# TODO maybe these comparisons should be folded into asSqlExpr
def asSqlCondition:
    def asSqlBinaryComparison(op): nullOr(
        if type == "array" and length == 2 then
            "\(.[0] | asSqlExpr) \(op) \(.[1] | asSqlExpr)"
        else error("Comparison '\(op)' expects exactly two expressions as an array, but found: \(tostring)")
        end);
    (.eq | asSqlBinaryComparison("=" )) //
    (.gt | asSqlBinaryComparison(">" )) //
    (.ge | asSqlBinaryComparison(">=")) //
    (.le | asSqlBinaryComparison("<=")) //
    (.lt | asSqlBinaryComparison("<" )) //
    (.isNull   | nullOr("\(asSqlExpr) IS NULL")) //
    (.isntNull | nullOr("\(asSqlExpr) IS NOT NULL")) //
    error("Unrecognized SQL condition \(tostring)");
# a more structured way to generate a SQL (Structured! Query Language) SELECT query than assembling strings
# which turns an object in a particular format into SQL, taking care of many escaping issues
def asSql:
    [ (.SELECT  |mapJoinOrEmptyString("SELECT "   ; asSqlExprAlias                                      ; "\n     , "))
    , (.FROM    |mapJoinOrEmptyString("FROM "     ; asSqlTableAlias(asSql)                              ; ", "))
    , (.JOIN    |mapJoinOrEmptyString(""; "\(asSqlJoinTypeTableAlias(asSql)) ON \(.ON | asSqlCondition)"; " " ))
    , (.WHERE   |mapJoinOrEmptyString("WHERE "    ; asSqlCondition                                      ; " AND " ))
    , (.GROUP_BY|mapJoinOrEmptyString("GROUP BY " ; asSqlExpr                                           ; ", "    ))
    , (.HAVING  |mapJoinOrEmptyString("HAVING "   ; asSqlCondition                                      ; " AND " ))
    , (.ORDER_BY|mapJoinOrEmptyString("ORDER BY " ; "\(.expr | asSqlExpr) \(.order // "ASC")"           ; ", "    ))
    ] | join("\n") | trimWhitespace;
## finally, a test case
#if
#    { SELECT:
#        [ { column: "id" }
#        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
#        , { expr: "COALESCE(initvalue, 0)" }
#        ]
#    , FROM:
#        [ { table:"dd_weights_foo" }
#        ]
#    } | asSql | debug |
#false then . else . end |
###############################################################################

.deepdive_ as $deepdive

# some global variables
| "id"                             as $deepdiveVariableIdColumn
| "dd_graph_variables_holdout"     as $deepdiveGlobalHoldoutTable
| "dd_graph_variables_observation" as $deepdiveGlobalObservationTable
| "dd_graph_weights"               as $deepdiveGlobalWeightsTable
| "dd_query_"                      as $deepdivePrefixForFactorsTable # TODO correct prefix to dd_factors_?
| "dd_weights_"                    as $deepdivePrefixForWeightsTable
| "dd_classes_"                    as $deepdivePrefixForMultinomialClassesTable

# parse variable schema to define some names (mainly to find Multinomial cardinality)
| .deepdive_.schema.variables_ = [
    $deepdive.schema.variables | to_entries[]
    | .key as $relationName | .value | to_entries
        | if length != 1 then error("deepdive.schema.variables.\($relationName) has \(length
            ) columns declared as variable. Every relation can have up to one column declared as a variable") else .[] end
        | .key as $columnName | .value as $variableType |
        { variableName         : $relationName
        , variablesTable       : $relationName
        , variablesLabelColumn : $columnName
        , variableClassesTable : "\($deepdivePrefixForMultinomialClassesTable)\($relationName)"
        } + (
            # parse variable type
            $variableType | trimWhitespace | ascii_downcase |
            capture("^(?:
                        (?<isBooleanType>     boolean     )
                      | (?<isCategoricalType> categorical )
                            \\s* \\(
                            \\s* (?<variableCardinality> \\d+)
                            \\s* \\)
                      )$"; "xmi") //
                error("deepdive.schema.variables.\($relationName).\($columnName
                    ) has an unrecognized type: \($variableType | @json)")
        ) |
        if   .isBooleanType     then .variableType = "boolean"     | .variableCardinality = 2
        elif .isCategoricalType then .variableType = "categorical" | .variableCardinality |= tonumber
        else .
        end
]
# create a map to make it easy to access a variable by its name
| .deepdive_.schema.variables_byName = (.deepdive_.schema.variables_ | map({key: .variableName, value: .}) | from_entries)

# parse inference rules, especially the function and weight fields as function_ and weight_
| .deepdive_.inference.factors_ = [
    $deepdive.inference.factors | to_entries[]
    | .key as $factorNameQualified
    | (.key | ltrimstr("factor/")) as $factorName | .value

    # some useful names for compilation
    | .factorNameQualified = $factorNameQualified
    | .factorName = $factorName
    | .factorsTable = "\($deepdivePrefixForFactorsTable)\($factorName)"
    | .weightsTable = "\($deepdivePrefixForWeightsTable)\($factorName)"

    # parse the weight field
    | .weight_ = (.weight | trimWhitespace
        | if startswith("?")? then
            # unknown weight, find parameters
            { is_fixed: false
            , params: (ltrimstr("?") | trimWhitespace
                | ltrimstr("(") | rtrimstr(")") | trimWhitespace
                | if length == 0 then [] else  split("\\s*,\\s*") end)
            , init_value: 0.0
            }
        else
            # fixed weight
            { is_fixed: true
            , params: []
            , init_value: tonumber
            }
        end
        )

    # parse the function field
    | .function_ = (.function | trimWhitespace
        | capture("^ (?<name>.+)
                \\s* \\(
                \\s* (?<variables>.+)
                \\s* \\)
                   $"; "x") // error("deepdive.inference.factors.\($factorName
                        ) has an unrecognized function: \(@json)")
        | .variables |= [ trimWhitespace | splits("\\s*,\\s*")
            # parse arguments to the function or predicate (variables)
            | capture("^ (?<isNegated>!)?
                    \\s* (?<relation>.+)
                    \\s* \\.
                    \\s* (?<field>[^.]+)
                    \\s* (?<isArray>\\[\\])?
                    \\s* (?: = (?<equalsTo> \\d+))?
                       $"; "x") // error("deepdive.inference.factors.\($factorName
                            ) has an unrecognized variable argument: \(@json)")
            | .isNegated |= (length > 0)
            | .isArray   |= (length > 0)
            | .equalsTo  |= (if . then tonumber else null end)
            ]
        | .id =
            # map function name to the code used by sampler (case insensitive)
            { imply       : 0
            , or          : 1
            , and         : 2
            , equal       : 3
            , istrue      : 4
            , multinomial : 5
            , linear      : 7
            , ratio       : 8
            , logical     : 9
            , imply3      : 11
            }[.name | ascii_downcase] //
                error("deepdive.inference.factors.\($factorName
                    ) uses an unrecognized function: \(.name | @json)")
        )
]
# create a map to make it easy to access a factor by its name
| .deepdive_.inference.factors_byName = (.deepdive_.inference.factors_ | map({key: .factorName, value: .}) | from_entries)

| .deepdive_ as $deepdive  # necessary since we just mutated it


| .deepdive_.extraction.extractors += {

    # grounding the factor graph
    # TODO remove me
    "process/grounding/legacy": {
        dependencies_: ($deepdive.inference.factors_ | map(.factorNameQualified)),
        output_: "model/factorgraph.legacy",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            mkdir -p factorgraph

            set +x; . load-db-driver.sh; set -x
            export DEEPDIVE_LOGFILE=factorgraph/grounding.log
            [[ ! -e \"$DEEPDIVE_LOGFILE\" ]] || mv -f \"$DEEPDIVE_LOGFILE\" \"$DEEPDIVE_LOGFILE\"~
            java org.deepdive.Main -c <(
                set +x
                echo \("deepdive \(.deepdive | @json)" | @sh)
                echo \("deepdive.pipeline.pipelines.grounding: [\($deepdive.inference.factors_ | map(.factorName) | join(", "))]" | @sh)
                echo \("deepdive.pipeline.run: grounding" | @sh)
            ) -o factorgraph -t inference_grounding

            # drop graph. prefix from file names
            cd factorgraph
            mv -f graph.variables variables
            mv -f graph.factors   factors
            mv -f graph.weights   weights
            mv -f graph.meta      meta
        "
    },

    # first of all,
    # consecutive variable id range should be partitioned first by counting the variables
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all variable tables
            $deepdive.schema.variables_[] | "data/\(.variablesTable)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables_ | map(.variablesTable | @sh) | join(" ")) | {
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    },

    # each variable gets the consecutive ids assigned to its rows
    # (process/grounding/variable/*/assign_id)

    # app-wide holdout query is executed
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables_[] | "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        for sql in \(
            [ " DROP TABLE IF EXISTS \($deepdiveGlobalHoldoutTable | asSqlIdent) CASCADE;
              CREATE TABLE \($deepdiveGlobalHoldoutTable | asSqlIdent)(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.holdout_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else
                # otherwise, randomly select from evidence variables of each variable table
                $deepdive.schema.variables_[] | "
                    INSERT INTO \($deepdiveGlobalHoldoutTable | asSqlIdent) \(
                    { SELECT:
                        [ { column: $deepdiveVariableIdColumn }
                        ]
                    , FROM:
                        [ { table: .variablesTable }
                        ]
                    , WHERE:
                        [ { isntNull: { column: .variablesLabelColumn } }
                        , { lt: [ { expr: "RANDOM()" }
                                , { expr: $deepdive.calibration.holdout_fraction }
                                ]
                          }
                        ]
                    } | asSql);
                "
              end
            , " DROP TABLE IF EXISTS \($deepdiveGlobalObservationTable | asSqlIdent) CASCADE;
              CREATE TABLE \($deepdiveGlobalObservationTable | asSqlIdent)(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.observation_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else empty
              end
            ] | map(@sh) | join(" "))
        do deepdive sql \"$sql\"
        done
        "
    },

    # then variables are dumped into tsv
    # (See process/grounding/variable/*/dump below)

    # each inference rule input_query is run to materialize the factors and the distinct weights used in them
    # (See process/grounding/factor/*/materialize below)

    # in between the two steps for grounding all factors, weight id range must be decided serially
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.inference.factors_ | map(.weightsTable | @sh) | join(" ")) | {
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=${table#\($deepdivePrefixForWeightsTable | @sh)}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }

        # set up a union view for all weight tables (\($deepdiveGlobalWeightsTable | asSqlIdent))
        deepdive sql \("DROP TABLE IF EXISTS \($deepdiveGlobalWeightsTable | asSqlIdent) CASCADE;" | @sh) || true
        deepdive sql \("
            DROP VIEW IF EXISTS \($deepdiveGlobalWeightsTable | asSqlIdent) CASCADE;
            CREATE VIEW \($deepdiveGlobalWeightsTable | asSqlIdent) AS \(
                [ $deepdive.inference.factors_[] |
                    { SELECT:
                        [ { column: "id" }
                        , { column: "isfixed" }
                        , { column: "initvalue" }
                        , { alias: "description", expr:
                                [ ("\(.factorName)-" | asSqlLiteral)
                                , (.weight_.params[] |
                                    "CASE WHEN \(asSqlIdent) IS NULL THEN ''
                                          ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
                                      END"
                                  )
                                ] | join(" ||\("-" | asSqlLiteral)|| ")
                          }
                        # TODO cardinality (actually class) column for multinomial as an offset to the id
                        ]
                    , FROM:
                        # TODO use exploded view for multinomial
                        [ { table: .weightsTable }
                        ]
                    } | asSql | "(\(.))"
                ] | join(" UNION ALL ")
            );" | @sh)
        "
    },

    # each inference rule gets weight ids actually assigned
    # (See process/grounding/factor/*/assign_weight_id below)

    # and the factors and weights are dumped into tsv
    # (See process/grounding/factor/*/dump below)

    # at the very end, everything grounded must be laid down in a format the sampler can load from
    "process/grounding/combine": {
        dependencies_: [(
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/dump"
        ), (
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/dump"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \([$deepdive.schema.variables_[] | .variableName | @sh] | join(" ")); do
            mkdir -p variables/\"$v\"
            ( cd variables/\"$v\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\"/variables.part-*.bin . )
        done
        for f in \([$deepdive.inference.factors_[] | .factorName | @sh] | join(" ")); do
            mkdir -p {factors,weights}/\"$f\"
            ( cd factors/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/factors.part-*.bin . )
            ( cd weights/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/weights.part-*.bin . )
        done

        # generate the metadata for the sampler
        {
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            # first line with counts of variables and edges in the graph
            counts=()
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/variable_count))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nfactors.part-* | sumup))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nedges.part-*   | sumup))
            IFS=,; echo \"${counts[*]}\"
            # second line with file paths
            paths=(\"$PWD\"/{weights,variables,factors,edges})
            IFS=,; echo \"${paths[*]}\"
        } >meta
        "
    }

}

# for each variable add some processes for grounding
| .deepdive_.extraction.extractors += merge($deepdive.schema.variables_[] | {

    # a process for assigning id to every variable according to the partition
    "process/grounding/variable/\(.variableName)/assign_id": {
        dependencies_: [
            "process/grounding/variable_id_partition"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        table=\(.variablesTable | @sh)

        cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
        baseId=$(cat id_begin)

        # assign id to all rows according to the paritition
        db-assign_sequential_id $table \($deepdiveVariableIdColumn | @sh) $baseId

        \(
        if .variableType == "categorical" then
            "
            # generate a table holding all classes for categorical variables
            # that can be cross joined with weights tables for factors over
            # those variables
            db-generate_series \(.variableClassesTable | @sh) 0 \(.variableCardinality - 1)
            "
        else "" end
        )
        "
    },

    # TODO easier way to do holdout per variable

    # a process for dumping each variable table
    "process/grounding/variable/\(.variableName)/dump": {
        dependencies_: [
            "process/grounding/variable_holdout"
          # XXX below can be omitted for now
          #, "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        table=\(.variablesTable | @sh)

        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        rm -f variables-*.bin
        export DEEPDIVE_LOAD_FORMAT=tsv

        # dump the variables, joining the holdout query to determine the type of each variable
        deepdive compute \\
            input_sql=\(
            { SELECT:
                [ { column: "id" }
                , { column: "variable_role" }
                , { alias: "init_value", expr:
                    "CASE WHEN variable_role = 0 THEN 0
                          ELSE (\(
                            if   .variableType == "boolean"     then "CASE WHEN label THEN 1 ELSE 0 END" # XXX a portable way to turn boolean to integers in SQL, CAST(label AS INT) does not work for MySQL
                            elif .variableType == "categorical" then "label"
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end
                            )) + 0.0
                      END" }
                , { column: "variable_type" }
                , { column: "cardinality" }
                ]
            , FROM:
                [ { alias: "variables", sql:
                    { SELECT:
                        [ { alias: "id", column: $deepdiveVariableIdColumn }
                        , { alias: "variable_role", expr:
                              "CASE WHEN               observation.variable_id IS NOT NULL
                                     AND variables.\(.variablesLabelColumn | asSqlIdent) IS NOT NULL THEN 2
                                    WHEN               holdout.variable_id IS NOT NULL THEN 0
                                    WHEN variables.\(.variablesLabelColumn | asSqlIdent) IS NOT NULL THEN 1
                                                                                       ELSE 0
                                END" }
                        , { alias: "label", table: "variables", column: .variablesLabelColumn }
                        , { alias: "variable_type", expr: (
                                if   .variableType == "boolean"     then 0
                                elif .variableType == "categorical" then 1
                                else error("Internal error: Unknown variableType: \(.variableType)")
                                end
                            ) }
                        , { alias: "cardinality", expr: (
                                .variableCardinality
                            ) }
                        ]
                    , FROM:
                        [ { alias: "variables", table: .variablesTable }
                        ]
                    , JOIN:
                        [ { LEFT_OUTER:
                            { alias: "holdout"
                            , table: $deepdiveGlobalHoldoutTable
                            }
                          , ON: { eq:
                                    [ { table: "variables", column: $deepdiveVariableIdColumn }
                                    , { table: "holdout"  , column: "variable_id" }
                                    ]
                                }
                          }
                        , { LEFT_OUTER:
                            { alias: "observation"
                            , table: $deepdiveGlobalObservationTable
                            }
                          , ON: { eq:
                                    [ { table: "variables"  , column: $deepdiveVariableIdColumn }
                                    , { table: "observation", column: "variable_id" }
                                    ]
                                }
                          }
                        ]
                    }
                  }
                ]
            } | asSql | @sh) \\
            command=\("
                format_converter variable /dev/stdin variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
            " | @sh) \\
            output_relation=
        "
    }

})


# for each inference rule, add some processes for grounding the factors and weights
| .deepdive_.extraction.extractors += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors
    "process/grounding/factor/\(.factorName)/materialize": {
        # materializing each factor requires the dependent variables to have their id assigned
        dependencies_: [
            .input_[]
            | ltrimstr("data/")
            | $deepdive.schema.variables_byName[.]
            | select(type != "null")
            # the involved variables must have their ids all assigned
            | "process/grounding/variable/\(.variableName)/assign_id"?
        ],
        # other non-variable tables are also necessary
        input_: [ .input_[]
            | select(ltrimstr("data/") | in($deepdive.schema.variables_byName) | not)
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            # materialize factors using user input_query that pulls in assigned ids to involved variables
            deepdive sql \("
                DROP TABLE IF EXISTS \(.factorsTable) CASCADE;
                CREATE TABLE \(.factorsTable) AS
                    \(.input_query);
            " | @sh)

            # find distinct weights for the factors into a separate table
            \("
            deepdive sql \("DROP VIEW IF EXISTS \(.weightsTable) CASCADE;" | @sh) || true
            deepdive sql \("
                DROP TABLE IF EXISTS \(.weightsTable) CASCADE;
                CREATE TABLE \(.weightsTable) AS \(
                    { SELECT:
                        [ ( .weight_.params[] | { column: . } )
                        , { expr: .weight_.is_fixed  , alias: "isfixed" }
                        , { expr: .weight_.init_value, alias: "initvalue" }
                        , { expr: -1                 , alias: "id" }  # TODO cast to BIGINT?
                        ]
                    # when weight is parameterized, find all distinct ones
                    , FROM:
                        (if .weight_.params | length == 0 then [] else
                            [ { table: .factorsTable }
                            ]
                        end)
                    , GROUP_BY:
                        (if .weight_.params | length == 0 then [] else
                            [ ( .weight_.params[] | { column: . } )
                            ]
                        end)
                    } | asSql);
            " | @sh)
            ")
        "
    },

    # add a process for grounding weights per inference rule
    "process/grounding/factor/\(.factorName)/assign_weight_id": {
        dependencies_: [
            "process/grounding/weight_id_partition"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            baseId=$(cat weights_id_begin)
            \( # consider step size = product of cardinalities for multinomial factors
            if .function_.name == "multinomial" then
                "inc=\(
                    reduce .function_.variables[] as $var (1;
                        . * $deepdive.schema.variables_byName[$var.relation].variableCardinality)
                )"
            else
                "inc=1"
            end
            )

            # assign weight ids according to the partition
            db-assign_sequential_id \(.weightsTable | @sh) id $baseId $inc
        "
    },

    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            rm -f {factors,weights}.part-*.bin n{factors,edges}.part-*
            export DEEPDIVE_LOAD_FORMAT=tsv

            # dump the factors joining the assigned weight ids, converting into binary format for the sampler
            DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.factors\" \\
            deepdive compute \\
                input_sql=\(
                    { SELECT:
                        [ { table: "weights", column: "id", alias: "weight_id" }
                        , ( .function_.variables[] |
                            { table: "factors", column: "\(.relation).\($deepdiveVariableIdColumn)" }
                          )
                        ]
                    , FROM:
                        [ { table: .factorsTable, alias: "factors" }
                        , { table: .weightsTable, alias: "weights" }
                        ]
                    , WHERE:
                        [ .weight_.params[] |
                            { eq: [ { table: "factors", column: . }
                                  , { table: "weights", column: . }
                                  ]
                            }
                        ]
                    } | asSql | @sh) \\
                command=\("
                    # also record the factor count
                    tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                    format_converter factor /dev/stdin factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin \(.function_.id
                        ) \(.function_.variables | length
                        ) original \(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ")
                        ) |
                    # and the edge count
                    tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                " | @sh) \\
                output_relation=

            \(
            # TODO set up a view cross joining the base weights table with the classes of categorical variables
            ""
            )

            # dump the weights (except the description column), converting into binary format for the sampler
            DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.weights\" \\
            deepdive compute \\
                input_sql=\(
                    { SELECT:
                        [ { column: "id" }
                        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(initvalue, 0)" }
                        ]
                    , FROM: [ { table: .weightsTable } ]
                    # TODO use exploded view with all combination of multinomial classes
                    } | asSql | @sh) \\
                command=\("
                    format_converter weight /dev/stdin weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
                " | @sh) \\
                output_relation=
        "
    }

})

