#!/usr/bin/env jq -f
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

def merge(objects): reduce objects as $es ({}; . + $es);

def trimWhitespace: gsub("^\\s+|\\s+$"; "");

# See: http://www.postgresql.org/docs/current/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
def asSqlIdent: "\"\( tostring  # anything can be turned into a SQL identifier if they are inside double quotes
                    | gsub("\""; "\"\"") # even double quote themselves (in fact, PostgreSQL says \0 cannot be included)
                    )\"";

# a handy way to turn things into SQL string literals (as it is hard to write single quote within single quote in this Bash script)
def asSqlLiteral: "'\(tostring | gsub("'"; "''"))'";

.deepdive_ as $deepdive
| "id" as $deepdiveVariableIdColumn

# parse function and weight fields for every inference rules as function_ and weight_
| .deepdive_.inference.factors |= with_entries(
    (.key | ltrimstr("factor/")) as $factorName | .value |=

    # parse the weight field
    ( .weight_ = (.weight | trimWhitespace |
        if startswith("?")? then
            # unknown weight, find parameters
            { is_fixed: false
            , params: (ltrimstr("?") | trimWhitespace
                | ltrimstr("(") | rtrimstr(")") | trimWhitespace
                | if length == 0 then [] else  split("\\s*,\\s*") end)
            , init_value: 0.0
            }
        else
            # fixed weight
            { is_fixed: true
            , params: []
            , init_value: tonumber
            }
        end)

    # parse the function field
    | .function_ = (.function | trimWhitespace
        | capture("^ (?<name>.+)
                \\s* \\(
                \\s* (?<variables>.+)
                \\s* \\)
                   $"; "x") // error("\(.): Failed parsing function field for deepdive.inference.factors.\($factorName)")
        | .variables |= [ trimWhitespace | splits("\\s*,\\s*")
            # parse arguments to the function or predicate (variables)
            | capture("^ (?<isNegated>!)?
                    \\s* (?<relation>.+)
                    \\s* \\.
                    \\s* (?<field>[^.]+)
                    \\s* (?<isArray>\\[\\])?
                    \\s* (?: = (?<equalsTo> \\d+))?
                       $"; "x") // error("\(.): Failed parsing variable argument for function for deepdive.inference.factors.\($factorName)")
            | .isNegated |= (length > 0)
            | .isArray   |= (length > 0)
            | .equalsTo  |= (if . then tonumber else null end)
            ]
        | .id =
            # map function name to the code used by sampler (case insensitive)
            { imply       : 0
            , or          : 1
            , and         : 2
            , equal       : 3
            , istrue      : 4
            , multinomial : 5
            , linear      : 7
            , ratio       : 8
            , logical     : 9
            , imply3      : 11
            }[.name | ascii_downcase] //
                error("\(.name): deepdive.inference.factors.\($factorName) uses an unrecognized function")
        )
    )
)

| .deepdive_ as $deepdive  # necessary since we just mutated it


| .deepdive_.extraction.extractors += {

    # grounding the factor graph
    "process/grounding/legacy": {
        dependencies_: ($deepdive.inference.factors | keys),
        output_: "model/factorgraph.legacy",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            mkdir -p factorgraph

            set +x; . load-db-driver.sh; set -x
            export DEEPDIVE_LOGFILE=factorgraph/grounding.log
            [[ ! -e \"$DEEPDIVE_LOGFILE\" ]] || mv -f \"$DEEPDIVE_LOGFILE\" \"$DEEPDIVE_LOGFILE\"~
            java org.deepdive.Main -c <(
                set +x
                echo \("deepdive \(.deepdive | @json)" | @sh)
                echo \("deepdive.pipeline.pipelines.grounding: [\(.deepdive.inference.factors | keys | join(", "))]" | @sh)
                echo \("deepdive.pipeline.run: grounding" | @sh)
            ) -o factorgraph -t inference_grounding

            # drop graph. prefix from file names
            cd factorgraph
            mv -f graph.variables variables
            mv -f graph.factors   factors
            mv -f graph.weights   weights
            mv -f graph.meta      meta
        "
    },

    # consecutive variable id range should be partitioned first by counting the variables
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all variable tables
            $deepdive.schema.variables | keys[] | "data/\(.)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables | keys | map(@sh) | join(" ")) | {
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    },

    # each variable gets the consecutive ids assigned to its rows (below)
    # app-wide holdout query is executed
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables | keys[] | "process/grounding/variable/\(.)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        for sql in \(
            [ " DROP TABLE IF EXISTS dd_graph_variables_holdout CASCADE;
              CREATE TABLE dd_graph_variables_holdout(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.holdout_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else
                # otherwise, randomly select from evidence variables of each variable table
                $deepdive.schema.variables | to_entries[]
                | .key as $relationName | .value | to_entries[] | .key as $columnName | "
                    INSERT INTO dd_graph_variables_holdout
                    SELECT \($deepdiveVariableIdColumn | asSqlIdent)
                      FROM \($relationName | asSqlIdent)
                     WHERE \($columnName | asSqlIdent) IS NOT NULL
                       AND RANDOM() < \($deepdive.calibration.holdout_fraction)
                    ;
                "
              end
            , " DROP TABLE IF EXISTS dd_graph_variables_observation CASCADE;
              CREATE TABLE dd_graph_variables_observation(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.observation_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else empty end
            ] | map(@sh) | join(" "))
        do deepdive sql \"$sql\"
        done
        "
    },
    # then variables are dumped into tsv (below)

    # each inference rule input_query is run to materialize the factors and the distinct weights used in them

    # in between the two steps for grounding all factors, weight id range must be decided serially
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors | keys[]
            | "process/grounding/\(.)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.inference.factors | keys | map(
                ltrimstr("factor/") | "dd_weights_\(.)" | @sh) | join(" ")) | {
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=${table#dd_weights_}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }

        # set up a union view for all weight tables (dd_graph_weights)
        deepdive sql \("DROP TABLE IF EXISTS dd_graph_weights CASCADE;" | @sh) || true
        deepdive sql \("
            DROP VIEW IF EXISTS dd_graph_weights CASCADE;
            CREATE VIEW dd_graph_weights AS \($deepdive.inference.factors | to_entries | map(
                    (.key | ltrimstr("factor/")) as $factorName | .value
                    | ( [ ("\($factorName)-" | asSqlLiteral)
                        , (.weight_.params[] | "CASE WHEN \(asSqlIdent) IS NULL THEN '' ELSE CAST(\(asSqlIdent) AS TEXT) END")
                        ] | join(" ||\("-" | asSqlLiteral)|| ") | "\(.) AS description") as $weightDescExpr
                    | "(SELECT id, isfixed, initvalue, \($weightDescExpr) FROM dd_weights_\($factorName))"
                    # TODO cardinality column for multinomial
                ) | join(" UNION ALL ")
            );" | @sh)
        "
    },

    # each inference rule gets weight ids actually assigned and the factors and weights are dumped into tsv (below)

    # at the very end, everything grounded must be laid down in a format the sampler can load from
    "process/grounding/combine": {
        dependencies_: [(
            $deepdive.schema.variables | to_entries[]
            | .key as $relationName | .value | keys[] | . as $columnName
            | "process/grounding/variable/\($relationName)/dump"
        ), (
            $deepdive.inference.factors | keys[] | ltrimstr("factor/")
            | "process/grounding/factor/\(.)/dump"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \([$deepdive.schema.variables | keys[] | @sh] | join(" ")); do
            mkdir -p variables/\"$v\"
            ( cd variables/\"$v\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\"/variables.part-*.bin . )
        done
        for f in \([$deepdive.inference.factors | keys[] | ltrimstr("factor/") | @sh] | join(" ")); do
            mkdir -p {factors,weights}/\"$f\"
            ( cd factors/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/factors.part-*.bin . )
            ( cd weights/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/weights.part-*.bin . )
        done

        # generate the metadata for the sampler
        {
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            # first line with counts of variables and edges in the graph
            counts=()
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/variable_count))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nfactors.part-* | sumup))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nedges.part-*   | sumup))
            IFS=,; echo \"${counts[*]}\"
            # second line with file paths
            paths=(\"$PWD\"/{weights,variables,factors,edges})
            IFS=,; echo \"${paths[*]}\"
        } >meta
        "
    }

}

# for each variable add some processes for grounding
| .deepdive_.extraction.extractors += merge($deepdive.schema.variables | to_entries[]
    | .key                   as $relationName
    | .value | to_entries[]
        | .key               as $columnName
        | .value             as $varType
        | "\($relationName)" as $varName
        # TODO handle $varType == Multinomial specially
        | {

            # a process for assigning id to every variable according to the partition
            "process/grounding/variable/\($varName)/assign_id": {
                dependencies_: [
                    "process/grounding/variable_id_partition"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                baseId=$(cat id_begin)

                # assign id to all rows according to the paritition
                db-assign_sequential_id $table \($deepdiveVariableIdColumn | @sh) $baseId
                "
            },

            # TODO easier way to do holdout per variable

            # a process for dumping each variable table
            "process/grounding/variable/\($varName)/dump": {
                dependencies_: [
                    "process/grounding/variable_holdout"
                  # XXX below can be omitted for now
                  #, "process/grounding/variable/\($varName)/assign_id"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\($varName | @sh)
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                rm -f variables-*.bin
                export DEEPDIVE_LOAD_FORMAT=tsv

                # dump the variables, joining the holdout query to determine the type of each variable
                deepdive compute \\
                    input_sql=\("
                    SELECT id
                         , variable_role
                         , CASE WHEN variable_role = 0 THEN 0
                                ELSE CAST(CAST(label AS INT) AS FLOAT)
                            END AS init_value
                         , variable_type
                         , cardinality
                      FROM (
                          SELECT \($deepdiveVariableIdColumn | asSqlIdent) AS id
                             , CASE WHEN               observation.variable_id IS NOT NULL
                                     AND variables.\($columnName | asSqlIdent) IS NOT NULL THEN 2
                                    WHEN                   holdout.variable_id IS NOT NULL THEN 0
                                    WHEN variables.\($columnName | asSqlIdent) IS NOT NULL THEN 1
                                                                                           ELSE 0
                               END AS variable_role
                             , variables.\($columnName | asSqlIdent) AS label
                             , \(
                                    # TODO multinomial
                                    0  # for Boolean variables
                                ) AS variable_type
                             , \(
                                    # TODO multinomial
                                    2
                                ) AS cardinality
                          FROM            \($relationName | asSqlIdent)  variables
                          LEFT OUTER JOIN dd_graph_variables_holdout     holdout     ON variables.\($deepdiveVariableIdColumn | asSqlIdent) =     holdout.variable_id
                          LEFT OUTER JOIN dd_graph_variables_observation observation ON variables.\($deepdiveVariableIdColumn | asSqlIdent) = observation.variable_id
                      ) variables
                    " | @sh) \\
                    command=\("
                        format_converter variable /dev/stdin variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
                    " | @sh) \\
                    output_relation=
                "
            }

            # TODO cardinality setup for multinomial

        })


# for each inference rule, add some processes for grounding the factors and weights
| .deepdive_.extraction.extractors += merge($deepdive.inference.factors | to_entries[]
    | (.key | ltrimstr("factor/")) as $factorName | .value
    | {
        # add a process for grounding factors
        "process/grounding/factor/\($factorName)/materialize": {
            # materializing each factor requires the dependent variables to have their id assigned
            dependencies_: [
                .input_[]
                | ltrimstr("data/") as $relationName
                | $deepdive.schema.variables[$relationName] | keys? | .[]
                | . as $columnName
                | "process/grounding/variable/\($relationName)/assign_id"
            ],
            # other non-variable tables are also necessary
            input_: [ .input_[]
                | select(ltrimstr("data/") | $deepdive.schema.variables[.] | not)
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                # materialize user input_query for the factor into a dd_query_* table
                deepdive sql \("dd_query_\($factorName)" as $factorTable | "
                    DROP TABLE IF EXISTS \($factorTable) CASCADE;
                    CREATE TABLE \($factorTable) AS
                        \(.input_query);
                " | @sh)

                # find distinct weights for the factor into a dd_weights_* table
                \(("dd_weights_\($factorName)" | asSqlIdent) as $weightsTable | "
                deepdive sql \("DROP VIEW IF EXISTS \($weightsTable) CASCADE;" | @sh) || true
                deepdive sql \((if .weight_.is_fixed then [] else .weight_.params | map(asSqlIdent) end) as $paramsAsSqlIdents | "
                    DROP TABLE IF EXISTS \($weightsTable) CASCADE;
                    CREATE TABLE \($weightsTable) AS
                        SELECT \(
                            [ $paramsAsSqlIdents[]
                            , "\(.weight_.is_fixed  ) AS isfixed"
                            , "\(.weight_.init_value) AS initvalue"
                            # TODO cast to BIGINT
                            , "-1                     AS id"
                            ] | join(", "))
                         \(if $paramsAsSqlIdents | length == 0 then ""
                         else "FROM dd_query_\($factorName)
                              GROUP BY \($paramsAsSqlIdents | join(", "))"
                         end);
                " | @sh)
                ")
            "
        },

        # add a process for grounding weights per inference rule
        "process/grounding/factor/\($factorName)/assign_weight_id": {
            dependencies_: [
                "process/grounding/weight_id_partition"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\($factorName | @sh)
                baseId=$(cat weights_id_begin)

                # assign weight ids according to the partition
                db-assign_sequential_id \("dd_weights_\($factorName)" | @sh) id $baseId
            "
        },

        # add a process for grounding factors and weights
        "process/grounding/factor/\($factorName)/dump": {
            dependencies_: [
                "process/grounding/factor/\($factorName)/assign_weight_id"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\($factorName | @sh)
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                rm -f {factors,weights}.part-*.bin n{factors,edges}.part-*
                export DEEPDIVE_LOAD_FORMAT=tsv

                # dump the factors joining the assigned weight ids, converting into binary format for the sampler
                DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.factors\" \\
                deepdive compute \\
                    input_sql=\("
                        SELECT weights.id AS weight_id
                            , \(.function_.variables | map("factors.\("\(.relation).\($deepdiveVariableIdColumn)" | asSqlIdent)") | join(", "))
                          FROM dd_query_\($factorName) factors
                             , dd_weights_\($factorName) weights
                        \(if .weight_.params | length == 0 then ""
                        else "WHERE \(.weight_.params | map(
                            "factors.\(asSqlIdent) = weights.\(asSqlIdent)"
                            ) | join(" AND "))"
                        end)
                    " | @sh) \\
                    command=\("
                        # also record the factor count
                        tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                        format_converter factor /dev/stdin factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin \(.function_.id
                            ) \(.function_.variables | length
                            ) original \(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ")
                            ) |
                        # and the edge count
                        tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                    " | @sh) \\
                    output_relation=

                # dump the weights (except the description column), converting into binary format for the sampler
                DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.weights\" \\
                deepdive compute \\
                    input_sql=\("
                        SELECT id
                             , CASE WHEN isfixed THEN 1 ELSE 0 END
                             , COALESCE(initvalue, 0)
                          FROM dd_weights_\($factorName)
                    " | @sh) \\
                    command=\("
                        format_converter weight /dev/stdin weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
                    " | @sh) \\
                    output_relation=
            "
        }

    })

