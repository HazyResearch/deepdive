#!/usr/bin/env jq
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

include "constants";
include "util";
include "sql";

# skip adding grounding processes unless there are variables and factors defined
if (.deepdive_.schema.variables_  | length) == 0
or (.deepdive_.inference.factors_ | length) == 0
then . else

def factorWeightDescriptionSqlExpr:
    # to serialize weight parameters describing each weight
    [ ("\(.factorName)-" | asSqlLiteral)
    , (.weight_.params[] |
        "CASE WHEN \(asSqlIdent) IS NULL THEN ''
              ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
          END"
      )
    ] | join(" ||\("-" | asSqlLiteral)|| ")
    ;
def multinomialFactorWeightCategoriesDescriptionSqlExpr:
    # to serialize category values describing an individual multinomial weight
    [ ( .function_.variables[]
    | .columnPrefix as $columnPrefixForThisVar
    | ( .schema.variablesCategoryColumns[]
    | "\($columnPrefixForThisVar)\(.)" | asSqlIdent )
    ) ] | join(" ||'\t'|| ")
    ;

.deepdive_ as $deepdive

###############################################################################

## variable/*/materialize
# Each internal variable table holding distinct variables should be
# materialized first for correct id assignment, etc.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/materialize": {
        dependencies_: [
            "process/grounding/from_grounding",
            "data/\(.variablesTable)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_GROUNDING_DENSE_MULTINOMIAL:=false}
        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"

        \(if .variableType == "categorical" then "
        # generate a table holding all distinct categories for categorical variables
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesCategoriesTable | @sh) as \(
        { SELECT:
            [ (.variablesCategoryColumns[]
            | { alias: "_\(.)", table: "v", column: . })
            , { alias: "count", expr: "COUNT(1)"             }
            , { alias: "cid"  , expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , GROUP_BY: [ .variablesCategoryColumns[] |         { table: "v", column: . }   ]
        , ORDER_BY: [ .variablesCategoryColumns[] | { expr: { table: "v", column: . } } ]
        } | asSql | asPrettySqlArg)
        # assign unique id to every distinct category
        deepdive db assign_sequential_id \(.variablesCategoriesTable | @sh) cid 0
        # TODO look at the count distribution to determine whether this is actually a dense categorical variable
        # for now, simply relying on a user environment to decide which representation to use for multinomial factors
        if $DEEPDIVE_GROUNDING_DENSE_MULTINOMIAL; then
            # mark this variable as dense so the rest of the grounding knows how to proceed
            touch isDenseCategorical
        else
            rm -f isDenseCategorical
        fi
        " else "" end)

        # generate a table holding all distinct variables identified by @key columns
        # so a unique id can be assigned to each of them
        deepdive create table \(.variablesIdsTable | @sh) as \(
        { SELECT:
            [ (.variablesKeyColumns[]
            | { alias: ., table: "v", column: . }) # XXX we don't prefix user's columns (with _ like others) since they directly use this table and it can cause confusion
            , { alias: deepdiveVariableInternalLabelColumn, expr:
                # boolean variables just take one label, so aggregate with AND
                ( if .variableType == "boolean" then "EVERY(\"v\".\(.variablesLabelColumn | asSqlIdent))"
                # categorical variables should point to the category id that has a true label column
                else "MIN(CASE WHEN \"v\".\(.variablesLabelColumn | asSqlIdent) THEN \"c\".\"cid\" ELSE NULL END)"
                end) }
            , { alias: deepdiveVariableInternalFrequencyColumn, expr: "COUNT(1)" }
            , { alias: deepdiveVariableIdColumn, expr: "CAST(NULL AS BIGINT)" }
            ]
        , FROM: [ { alias: "v", table: .variablesTable } ]
        , JOIN:
            ( if .variableType == "boolean" then null else
                # categorical variables need a join with the categories table
                # to find the correct internal label, etc.
                [ { INNER: { alias: "c", table: .variablesCategoriesTable }
                  , ON: { and: [ .variablesCategoryColumns[]
                               | { eq: [ { table: "c", column: "_\(.)" }
                                       , { table: "v", column: . } ]
                                 } ] }
                  } ]
            end)
        , GROUP_BY: [ .variablesKeyColumns[] | { column: . } ]
        } | asSql | asPrettySqlArg)
        "
    }
})

## variable_id_partition
# Grounding begins by counting the variables to partition a range of
# non-negative integers for assigning the variable ids.
| .deepdive_.execution.processes += {
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all distinct variables to be first identified
            $deepdive.schema.variables_[] | "process/grounding/variable/\(.variableName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables_ | map(.variablesIdsTable | @sh) | join(" ")) | {
            # variable names
            set -- \($deepdive.schema.variables_ | map(.variableName | @sh) | join(" "))
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varName=$1; shift
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${varName}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    }
}


## variable/*/assign_id
# Each variable table then gets the range of integers assigned to the id column
# of every row.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/assign_id": {
        dependencies_: [
            "process/grounding/variable_id_partition"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        baseId=$(cat id_begin)

        # assign id to all distinct variables according to the paritition
        deepdive db assign_sequential_id \(.variablesIdsTable | @sh) \(deepdiveVariableIdColumn | @sh) $baseId
        "
    }
})

## variable_holdout
# Variables to holdout are recorded by executing either a user-defined
# (app-wide) holdout query, or by taking a random sample of a user-defined
# fraction.
# TODO easier way to do holdout per variable?
| .deepdive_.execution.processes += {
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        deepdive create table \(deepdiveGlobalHoldoutTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        deepdive create table \(deepdiveGlobalObservationTable | @sh) \\
            variable_id:BIGINT:'PRIMARY KEY' \\
            #
        \([ if $deepdive.calibration.holdout_query then
            # run user holdout query if configured
            $deepdive.calibration.holdout_query
          else
            # otherwise, randomly select from evidence variables of each variable table
            $deepdive.schema.variables_[] | "
                INSERT INTO \(deepdiveGlobalHoldoutTable | asSqlIdent) \(
                { SELECT: [ { column: deepdiveVariableIdColumn } ]
                , FROM: [ { table: .variablesIdsTable } ]
                , WHERE:
                    [ { isntNull: { column: deepdiveVariableInternalLabelColumn } }
                    , { lt: [ { expr: "RANDOM()" }
                            , { expr: $deepdive.calibration.holdout_fraction }
                            ]
                      }
                    ]
                } | asSql);
            "
          end
        , if $deepdive.calibration.observation_query then
            # run user holdout query if configured
            $deepdive.calibration.holdout_query
          else empty
          end
        ] | map("deepdive sql \(asPrettySqlArg)") | join("\n"))
        "
    }
}

## variable/*/dump
# Then each variable table is dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[] | {
    "process/grounding/variable/\(.variableName)/dump": {
        dependencies_: [
            "process/grounding/variable_holdout"
          # XXX below can be omitted for now
          #, "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'variables.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        # dump the variables, joining the holdout query to determine the type of each variable
        deepdive compute execute \\
            input_sql=\(
            { SELECT:
                [ { column: "vid" }
                , { column: "variable_role" }
                , { alias: "init_value", expr:
                    "CASE WHEN variable_role = 0 THEN 0
                          ELSE (\(
                            if   .variableType == "boolean"     then "CASE WHEN label THEN 1 ELSE 0 END" # XXX a portable way to turn boolean to integers in SQL, CAST(label AS INT) does not work for MySQL
                            elif .variableType == "categorical" then "label"
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end
                            )) + 0.0
                      END" }
                , { column: "variable_type" }
                , { column: "cardinality" }
                ]
            , FROM: { alias: "variables", sql:
                { SELECT:
                    [ { alias: "vid", table: "i", column: deepdiveVariableIdColumn }
                    , { alias: "variable_role", expr:
                          "CASE WHEN observation.variable_id IS NOT NULL
                                 AND \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 2
                                WHEN holdout.variable_id IS NOT NULL THEN 0
                                WHEN \"i\".\(deepdiveVariableInternalLabelColumn | asSqlIdent) IS NOT NULL THEN 1
                                ELSE 0
                            END" }
                    , { alias: "label", table: "i", column: deepdiveVariableInternalLabelColumn }
                    , { alias: "variable_type", expr:
                            ( if .variableType == "boolean"     then 0
                            elif .variableType == "categorical" then 1
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    , { alias: "cardinality", expr:
                            ( if .variableType == "boolean"     then 2
                            elif .variableType == "categorical" then "\"i\".\"dd__count\"" # TODO count the number of actual distinct values by .variablesCategoryColumns for this row
                            else error("Internal error: Unknown variableType: \(.variableType)")
                            end) }
                    ]
                , FROM: { alias: "i", table: .variablesIdsTable }
                , JOIN:
                    [ { LEFT_OUTER: { alias: "holdout", table: deepdiveGlobalHoldoutTable }
                      , ON: { eq: [ { table: "i", column: deepdiveVariableIdColumn }
                                  , { table: "holdout"  , column: "variable_id" } ] } }
                    , { LEFT_OUTER: { alias: "observation", table: deepdiveGlobalObservationTable }
                      , ON: { eq: [ { table: "i"  , column: deepdiveVariableIdColumn }
                                  , { table: "observation", column: "variable_id" } ] } }
                    , ( if .variableType != "categorical" then empty else
                      { LEFT_OUTER: { alias: "categories" , table: .variablesCategoriesTable }
                      , ON: { eq: [ { table: "i"  , column: deepdiveVariableIdColumn }
                                  , { table: "observation", column: "variable_id" } ] } }
                      end)
                    ]
                } }
            } | asSql | asPrettySqlArg) \\
            command=\("
                sampler-text2bin variable /dev/stdin >(pbzip2 >variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        "
    }

})


## variable/*/dump_domains
# Each categorical variable dumps an extra input for the inference engine that holds the domains.
| .deepdive_.execution.processes += merge($deepdive.schema.variables_[]
    | select(.variableType == "categorical") | {
    "process/grounding/variable/\(.variableName)/dump_domains": {
        dependencies_: [
            "process/grounding/variable/\(.variableName)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\(.variableName | @sh)
        mkdir -p \"$varPath\"
        cd \"$varPath\"
        find . -name 'domains.part-*.bin.bz2' -exec rm -rf {} +
        export DEEPDIVE_LOAD_FORMAT=tsv
        export DEEPDIVE_UNLOAD_MATERIALIZED=false

        if [[ -e isDenseCategorical ]]; then
            # dense categorical variables don't need to dump their domains
            : no-op
        else
            # dump the categorical variable domains, joining the categories table for their ids
            deepdive compute execute \\
            input_sql=\(
            # sparse categorical variables have domains files
            { SELECT:
                [ { alias: "vid",  table: "i", column: deepdiveVariableIdColumn }
                , { alias: "cardinality", expr: "COUNT(c.cid)" }
                , { alias: "cids", expr: "ARRAY_AGG(c.cid ORDER BY c.cid)" }  # TODO move this ORDER BY into FROM? since it sounds like ARRAY_AGG(... ORDER BY ...) is PG 9+
                ]
            , FROM: [ { alias: "v", table: .variablesTable } ]
            , JOIN:
                # variable ids
                [ { INNER: { alias: "i", table: .variablesIdsTable }
                  , ON: { and:  [ .variablesKeyColumns[]
                                | { eq: [ { table: "i", column: . }
                                        , { table: "v", column: . }
                                        ] }
                                ] } }
                # category ids are necessary to find the inference result corresponding to the variable
                , { INNER: { alias: "c", table: .variablesCategoriesTable }
                  , ON: { and:  [ .variablesCategoryColumns[]
                                | { eq: [ { table: "c", column: "_\(.)" }
                                        , { table: "v", column: . }
                                        ] }
                                ] } }
                ]
            , GROUP_BY: [ { table: "i", column: deepdiveVariableIdColumn } ]
            } | asSql | asPrettySqlArg) \\
            command=\("
                sampler-text2bin domain /dev/stdin >(pbzip2 >domains.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
            " | @sh) \\
            output_relation=
        fi
        "
    }

})


###############################################################################

## factor/*/materialize
# Each inference rule's SQL query is run to materialize the factors and the
# distinct weights used in them.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors
    "process/grounding/factor/\(.factorName)/materialize": {
        # materializing each factor requires the dependent variables to have their id assigned
        dependencies_: [
            .input_[]
            | ltrimstr("data/")
            | $deepdive.schema.variables_byName[.]
            | select(type != "null")
            # the involved variables must have their ids all assigned
            | "process/grounding/variable/\(.variableName)/assign_id"?
        ],
        # other non-variable tables are also necessary
        input_: [ .input_[]
            | select(ltrimstr("data/") | in($deepdive.schema.variables_byName) | not)
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"

            # materialize factors using user input_query that pulls in assigned ids to involved variables
            deepdive create table \(.factorsTable | @sh) as \(
                # FIXME materializing the factor table is in fact unnecessary/not-so-critical/unhelpful
                # since we can evaluate it later directly while dumping, and finding all distinct weight parameters
                # uses a stricter projection but may lose the query optimization opportunity by this materialization
                { SELECT:
                    [ ( .function_.variables[] | { table: "Q", column: .columnId } )
                    , ( .weight_.params[]      | { table: "Q", column: . } )
                    ]
                , DISTINCT: true  # XXX maybe we should assume user is going to be make this true
                , FROM: [ { alias: "Q", sql: .input_query } ]
                } | asSql | asPrettySqlArg)

            \(( { SELECT:
                    # weight parameters
                    [ ( .weight_.params[] | { column: . } )
                    # weight attributes
                    , { alias: "isfixed"  , expr: .weight_.is_fixed      }
                    , { alias: "initvalue", expr: .weight_.init_value    }
                    , { alias: "wid"      , expr: "CAST(NULL AS BIGINT)" } # to be assigned later by the assign_weight_id process
                    ]
                , DISTINCT: true  # XXX this is inevitable
                , FROM:
                    [ select(.weight_.params | length > 0)
                    # when weight is parameterized, find all distinct ones
                    | { alias: "f", table: .factorsTable }
                    ]
                } | asSql | asPrettySqlArg) as $weightsMaterializeSqlArg |
            if .function_.isMultinomial then # factors over categorical variables
            "
            # use dense representation only if all variables of the factor are dense
            isDenseMultinomial=true
            for v in \([ .function_.variables[] | .schema.variableName | @sh ] | unique | join(" ")); do
                ! [[ -e \"$DEEPDIVE_GROUNDING_DIR/variable/$v/isDenseCategorical\" ]] || continue
                isDenseMultinomial=false
                break
            done
            if $isDenseMultinomial; then
                touch isDenseMultinomial
                # find distinct weight parameters for factors
                # without considering the exact combinations of category values,
                # assuming they'll all be useful to put in a dense representation
                deepdive create table \(.weightGroupsTable | @sh) as \($weightsMaterializeSqlArg)
            else
                rm -f isDenseMultinomial
                # find distinct weight parameters for factors
                # considering all the exact combintations of category values present in the factors
                # assuming they'll be sparse
                deepdive create table \(.weightsTable | @sh) as \(
                    # NOTE Instead of having to join the variable tables back with the factor table just materialized to find the
                    # exact category column combinations, we can simply start from the input query and project the needed columns.
                    { SELECT:
                        # weight parameters
                        [ ( .weight_.params[] | { table: "Q", column: . } )
                        # and the category columns of every categorical variable,
                        # projected from the factor's input_query following the naming convention
                        , ( .function_.variables[]
                        | .columnPrefix as $columnPrefixForThisVar
                        | .schema.variablesCategoryColumns[]
                        | { alias: "\($columnPrefixForThisVar)\(.)", table: "\($columnPrefixForThisVar)v", column: . } )
                        # weight attributes
                        , { alias: "isfixed"  , expr: .weight_.is_fixed      }
                        , { alias: "initvalue", expr: .weight_.init_value    }
                        , { alias: "wid"      , expr: "CAST(NULL AS BIGINT)" } # to be assigned later by the assign_weight_id process
                        ]
                    , DISTINCT: true  # XXX this is inevitable
                    , FROM: [ { alias: "Q", sql: .input_query } ]
                    , JOIN:
                        # FIXME To avoid these joins, it should be much better to rewrite the .input_query to project the category
                        # columns along with the weight parameters directly from the variable tables, rather than joining a reduced
                        # result back with the original variablesTables. However, this requires .input_query to be structured and
                        # hence a lot of change from both DDlog compiler and the user.
                        [ .function_.variables[]
                        | .columnPrefix as $columnPrefixForThisVar
                        | { INNER: { alias: "\($columnPrefixForThisVar)v", table: .schema.variablesTable }
                          , ON: [ .schema.variablesKeyColumns[]
                                | { eq: [ { table: "\($columnPrefixForThisVar)v", column: "\(.)" }
                                        , { table: "Q", column: "\($columnPrefixForThisVar)\(.)" }
                                        ] }
                                ] }
                        ]
                    # NOTE We need/want ORDER_BY here because we want the weight ids to be ordered exactly by their category
                    # values, so we can avoid global ordering of the entire join of factor and weight later when dumping the
                    # factors to produce a well ordered list of weight ids.
                    , ORDER_BY:
                        # order by category columns for every categorical variable
                        [ ( .function_.variables[]
                        | .columnPrefix as $columnPrefixForThisVar
                        | .schema.variablesCategoryColumns[]
                        | { expr: { column: "\($columnPrefixForThisVar)\(.)" } } )
                        ]
                    } | asSql | asPrettySqlArg)
            fi
            " else # factors over boolean variables
            "
            # find distinct weights for the factors into a separate table
            deepdive create table \(.weightsTable | @sh) as \($weightsMaterializeSqlArg)
            " end)
        "
    }
})


## weight_id_partition
# The weight ids must be first partitioned by counting them.
| .deepdive_.execution.processes += {
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        cd \"$DEEPDIVE_GROUNDING_DIR\"/factor

        \(if [ $deepdive.inference.factors_[].function_.isMultinomial ] | any then
            # there are multinomial factors in the model, need to check if each
            # will use dense representation to determine whether to use a step
            # size larger than 1 when partitioning the weight ids
            [ ( [ $deepdive.inference.factors_[].function_.variables[].schema ] | unique[]
            | "\(.variableName)Cardinality=$(deepdive sql eval \(
                { SELECT: { expr: "COUNT(*)" }
                , FROM: { table: .variablesCategoriesTable }
                } | asSql | asPrettySqlArg))" )
            , "weightsTablesToConsider=()"
            , ( $deepdive.inference.factors_[]
            | if .function_.isMultinomial then
                "
                if [[ -e \(.factorName | @sh)/isDenseMultinomial ]]; then
                    # dense multinomial should use a different step size
                    numWeightsPerFactor=$(bc <<<\"\(
                        [ "$\(.function_.variables[].schema.variableName)Cardinality" ] | join(" * ")
                    )\")
                    echo $numWeightsPerFactor >\(.factorName | @sh)/numWeightsPerFactor
                    weightsTablesToConsider+=(\(.weightGroupsTable | @sh):$numWeightsPerFactor)
                else
                    weightsTablesToConsider+=(\(.weightsTable | @sh))
                fi
                "
            else
                # non-multinomial factors
                "
                weightsTablesToConsider+=(\(.weightsTable | @sh))
                "
            end)
            , "set -- \"${weightsTablesToConsider[@]}\""
            ] | join("\n")
        else
            # no multinomial factors
            [ "set --", "\($deepdive.inference.factors_[].weightsTable | @sh)", "#" ] | join(" \\\n    ")
        end)

        # partition the id range for weights
        RANGE_BEGIN=0 RANGE_STEP=1 \\
        partition_id_range \"$@\" | {
            # factor names
            set -- \($deepdive.inference.factors_ | map(.factorName | @sh) | join(" "))
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=$1; shift
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }
        "
    }
}

## factor/*/assign_weight_id
# Each inference rule gets its weight ids actually assigned.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    "process/grounding/factor/\(.factorName)/assign_weight_id": {
        dependencies_: [
            "process/grounding/weight_id_partition"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

            cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            baseId=$(cat weights_id_begin)

            \( if .function_.isMultinomial then # multinomial factors
            "
            if [[ -e isDenseMultinomial ]]; then
                # dense multinomial should assign weight ids with an increment greater than 1
                # since each factor will have as many weights as the product of cardinalities of all involved variables
                increment=$(cat numWeightsPerFactor)
                deepdive db assign_sequential_id \(.weightGroupsTable | @sh) wid $baseId $increment

                # to keep dense multinomial weights in sync with the rest that assumes sparse representation
                # set up an exploded weights view that cross joins the base weights table with the categories of all involved variables
                deepdive create view \(.weightsTable | @sh) as \(
                { SELECT:
                    # weight parameters
                    [ ( .weight_.params[]
                    | { column: ., table: "wg" } )
                    # include category columns for every categorical variable
                    , ( .function_.variables[]
                    | .columnPrefix as $columnPrefixForThisVar
                    | .schema.variablesCategoryColumns[]
                    | { alias: "\($columnPrefixForThisVar)\(.)", table: "\($columnPrefixForThisVar)c", column: "_\(.)" } )
                    # weight attributes
                    , { column: "isfixed"  , table: "wg" }
                    , { column: "initvalue", table: "wg" }
                    # derive the individual weight id for every combination of categories
                    , { alias: "wid", expr: "\"wg\".\"wid\" + (\(
                            def offsetExpr(i):
                                if i == 0 then "" # except the very first variable
                                else # the offset for the previous ones are multiplied by this one's cardinality
                                    "(\(offsetExpr(i-1))) * (\(
                                        { SELECT: { expr: "COUNT(*)" }
                                        , FROM: { table: .[i].schema.variablesCategoriesTable}
                                        } | asSql)) + "
                                end
                                # and current variable's category id is added as the least significant digit
                                + ({ table: "\(.[i].columnPrefix)c", column: "cid" } | asSqlExpr)
                                ;
                            .function_.variables | offsetExpr(length-1)
                        ))" }
                    ]
                , FROM:
                    [ { alias: "wg", table: .weightGroupsTable }
                    # cross join each weight with the categories of all involved categorical variables
                    , ( .function_.variables[]
                    | .columnPrefix as $columnPrefixForThisVar
                    | { alias: "\($columnPrefixForThisVar)c", table: .schema.variablesCategoriesTable } )
                    ]
                # NOTE No ordering with the category columns is necessary here because the wid can be determined as a linear
                # combination of the corresponding cids.
                } | asSql | @sh)
            else
                # assign weight ids according to the partition as usual
                deepdive db assign_sequential_id \(.weightsTable | @sh) wid $baseId
            fi
            " else # boolean factors don't need special care
            "
            # assign weight ids according to the partition
            deepdive db assign_sequential_id \(.weightsTable | @sh) wid $baseId
            " end)
        "
    }
})

## global_weight_table
# To view the weights learned by the inference engine later, set up an app-wide table.
| .deepdive_.execution.processes += {
    "process/grounding/global_weight_table": {
        dependencies_: [
            $deepdive.inference.factors_[] |
            if .function_.isMultinomial then
                "process/grounding/factor/\(.factorName)/assign_weight_id"
            else
                "process/grounding/factor/\(.factorName)/materialize"
            end
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # set up a union view for all weight tables (\(deepdiveGlobalWeightsTable | asSqlIdent))
        deepdive create view \(deepdiveGlobalWeightsTable | @sh) as \(
            [ $deepdive.inference.factors_[] |
                { SELECT:
                    [ { column: "wid" }
                    , { column: "isfixed" }
                    , { column: "initvalue" }
                    , { alias: "description", expr: factorWeightDescriptionSqlExpr }
                    , if .function_.isMultinomial then
                        { alias: "categories", expr: multinomialFactorWeightCategoriesDescriptionSqlExpr }
                    else
                        { alias: "categories", expr: "NULL || ''" }
                    end
                    ]
                , FROM:
                    [ { table: .weightsTable }
                    ]
                } | asSql | "(\(.))"
            ] | join("\nUNION ALL\n") | asPrettySqlArg)
        "
    }
}

## factor/*/dump
# The factors are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'factors.part-*.bin.bz2' \\
                    -o -name 'nfactors.part-*'         \\
                    -o -name   'nedges.part-*'         \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # named codes used by sampler
            export \(.function_.name)Factor=\(.function_.id)  # factor function id
            \( if .function_.isMultinomial then "
            [[ -e isDenseMultinomial ]] || \(.function_.name)Factor=\(.function_.idForSparseVariant)  # use sparse variant"
            else ""
            end )
            export numVariablesForFactor=\(.function_.variables | length)
            export areVariablesPositive=\(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ") | @sh)

            # dump the factors joining the assigned weight ids, converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\(
                if .function_.isMultinomial then
                "\"$(
                if [[ -e isDenseMultinomial ]]; then
                    # dense multinomial factor needs not join the weight ids,
                    # since it exploits the fact that all possible weights are materialized, and
                    # each factor can use the variables' domain values to index into the exact one
                    echo \(
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        , { alias: "weight_id", table: "w", column: "wid" }
                        ]
                    , FROM:
                        [ { alias: "f", table: .factorsTable }
                        , { alias: "w", table: .weightGroupsTable }  # NOTE that we only dump the first weight id per factor referring to the entire group
                        ]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        ]
                    } | asSql | asPrettySqlArg)
                else
                    # sparse multinomial factor need to find exact combinations of category values and
                    # weight parameters present in the data, hence a lot of joins! but doesn't waste the weights
                    echo \(
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "Q", column: .columnId } )
                        , { alias: "num_weights", expr: "COUNT(w.wid)" }
                        # NOTE Putting an ORDER_BY here (in fact as a subquery for the GROUP_BY) is ludicruous since this is basically
                        # exploding the data and then sorting it just to make the weight id ARRAYs be ordered in a particular way.
                        # Keeping the weight ids ordered and relying on ARRAY_AGG(w.wid ORDER BY w.wid) is better.
                        , { alias: "weight_ids" , expr: "ARRAY_AGG(w.wid ORDER BY w.wid)" }
                        ]
                    , FROM: [ { alias: "Q", sql: .input_query } ]
                    , JOIN:
                        # FIXME similar to .weightsTable creation in */materialize process, these joins should go away with .input_query rewriting
                        [ ( .function_.variables[]
                        | .columnPrefix as $columnPrefixForThisVar
                        | { INNER: { alias: "\($columnPrefixForThisVar)v", table: .schema.variablesTable }
                          , ON: [ .schema.variablesKeyColumns[]
                                | { eq: [ { table: "\($columnPrefixForThisVar)v", column: . }
                                        , { table: "Q", column: "\($columnPrefixForThisVar)\(.)" }
                                        ] }
                                ] } )
                        # NOTE that this join is inevitable to find the weight ids tied to the factor
                        , { INNER: { alias: "w", table: .weightsTable }
                          , ON: [ ( .function_.variables[]
                                | .columnPrefix as $columnPrefixForThisVar
                                | ( .schema.variablesCategoryColumns[]
                                | { eq: [ { table: "w", column: "\($columnPrefixForThisVar)\(.)" }
                                        , { table: "\($columnPrefixForThisVar)v", column: . } ] } ) )
                                ] }
                        ]
                    , GROUP_BY:
                        [ ( .function_.variables[]
                        | { table: "Q", column: .columnId } )
                        ]
                    } | asSql | asPrettySqlArg)
                fi
                )\"" else # factors over boolean variables
                    { SELECT:
                        [ ( .function_.variables[]
                        | { table: "f", column: .columnId } )
                        , { alias: "weight_id", table: "w", column: "wid" }
                        ]
                    , FROM:
                        [ { alias: "f", table: .factorsTable }
                        , { alias: "w", table: .weightsTable }
                        ]
                    , WHERE:
                        [ ( .weight_.params[]
                        | { eq: [ { table: "w", column: . }
                                , { table: "f", column: . } ] } )
                        ]
                    } | asSql | asPrettySqlArg
                end) \\
                command=\("
                    # also record the factor count
                    tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                    sampler-text2bin factor /dev/stdin >(pbzip2 >factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2) \\
                        $\(.function_.name)Factor \\
                        $numVariablesForFactor \\
                        original \\
                        $areVariablesPositive |
                    # and the edge count
                    tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                " | @sh) \\
                output_relation=
        "
    }
})

## factor/*/dump_weights
# The factors and weights are dumped into a set of binary files for the inference engine.
| .deepdive_.execution.processes += merge($deepdive.inference.factors_[] | {
    # add a process for grounding factors and weights
    "process/grounding/factor/\(.factorName)/dump_weights": {
        dependencies_: [
            "process/grounding/factor/\(.factorName)/assign_weight_id"
        ],
        style: "cmd_extractor", cmd: "
            : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
            facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\(.factorName | @sh)
            mkdir -p \"$facPath\"
            cd \"$facPath\"
            find . \\( -name  'weights.part-*.bin.bz2' \\
                   \\) -exec rm -rf {} +
            export DEEPDIVE_LOAD_FORMAT=tsv
            export DEEPDIVE_UNLOAD_MATERIALIZED=false

            # flag that signals whether to reuse weights or not
            reuseFlag=\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights.reuse

            # dump the weights (except the description column), converting into binary format for the inference engine
            deepdive compute execute \\
                input_sql=\"$(if [[ -e \"$reuseFlag\" ]]; then
                    echo \(
                    # dump weights with initvalue from previously learned ones
                    { SELECT:
                        [ { table: "w", column: "wid" }
                        , { expr: "CASE WHEN w.isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(reuse.weight, w.initvalue, 0)" }
                        ]
                    , FROM: [ { alias: "w", table: .weightsTable } ]
                    , JOIN: { LEFT_OUTER: { alias: "reuse", table: deepdiveReuseWeightsTable }
                            , ON: { and: [ { eq: [ { table: "reuse", column: "description" }
                                                 , { expr: factorWeightDescriptionSqlExpr }
                                                 ] }
                                         , if .function_.isMultinomial then
                                           { or: [ { isNull: { table: "reuse", column: "categories" } }
                                                   , { eq: [ { table: "reuse", column: "categories" }
                                                           , { expr: multinomialFactorWeightCategoriesDescriptionSqlExpr }
                                                           ] }
                                                 ] }
                                           else empty end
                                         ] }
                            }
                    } | asSql | asPrettySqlArg)
                else
                    echo \(
                    # dump weights from scratch
                    { SELECT:
                        [ { column: "wid" }
                        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                        , { expr: "COALESCE(initvalue, 0)" }
                        ]
                    , FROM: [ { table: .weightsTable } ]
                    } | asSql | asPrettySqlArg)
                fi)\" \\
                command=\("
                    sampler-text2bin weight /dev/stdin >(pbzip2 >weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin.bz2)
                " | @sh) \\
                output_relation=
        "
    }
})

###############################################################################

# Finally, put together everything dumped into a layout the inference engine can easily load from
| .deepdive_.execution.processes += {
    "process/grounding/combine_factorgraph": {
        dependencies_: [(
            $deepdive.schema.variables_[]
            | "process/grounding/variable/\(.variableName)/dump"
            , (select(.variableType == "categorical")
            | "process/grounding/variable/\(.variableName)/dump_domains")
        ), (
            $deepdive.inference.factors_[]
            | "process/grounding/factor/\(.factorName)/dump"
            , "process/grounding/factor/\(.factorName)/dump_weights"
        ), (
            "process/grounding/global_weight_table"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: (
            ([$deepdive.schema.variables_[] | .variableName | @sh] | join(" ")) as $variableNames |
            ([$deepdive.inference.factors_[] | .factorName  | @sh] | join(" ")) as $factorNames   |
        "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        # create a fresh empty directory for the new combined factor graph
        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \($variableNames); do
            mkdir -p {variables,domains}/\"$v\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\" \\
                -name 'variables.part-*.bin.bz2' -exec ln -sfnv -t variables/\"$v\"/ {} + \\
                -o \\
                -name   'domains.part-*.bin.bz2' -exec ln -sfnv -t   domains/\"$v\"/ {} + \\
                #
        done
        for f in \($factorNames); do
            mkdir -p {factors,weights}/\"$f\"
            find \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\" \\
                -name 'factors.part-*.bin.bz2' -exec ln -sfnv -t factors/\"$f\"/ {} + \\
                -o \\
                -name 'weights.part-*.bin.bz2' -exec ln -sfnv -t weights/\"$f\"/ {} + \\
                #
        done

        # generate the metadata for the inference engine
        {
            # first line with counts of variables and edges in the grounded factor graph
            cd \"$DEEPDIVE_GROUNDING_DIR\"
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            counts=()
            counts+=($(cat factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat variable_count))
            cd factor
            counts+=($(find \($factorNames) -name 'nfactors.part-*' -exec cat {} + | sumup))
            counts+=($(find \($factorNames) -name 'nedges.part-*'   -exec cat {} + | sumup))
            (IFS=,; echo \"${counts[*]}\")
            # second line with file paths
            paths=(\"$DEEPDIVE_FACTORGRAPH_DIR\"/{weights,variables,factors,edges,domains})
            (IFS=,; echo \"${paths[*]}\")
        } >meta
        ")
    }
}

## from_grounding
# A nominal process to make it easy to redo the grounding
# TODO remove this once deepdive-do supports process groups or pipelines
| .deepdive_.execution.processes += {
    "process/grounding/from_grounding": {
        style: "cmd_extractor", cmd: ": no-op"
    }
}

end
