#!/usr/bin/env jq -f
# compile-config-2.01-grounding -- Adds processes for grounding the factor graph
##

###############################################################################
def merge(objects): reduce objects as $es ({}; . + $es);

def trimWhitespace: gsub("^\\s+|\\s+$"; ""; "m");

def nullOr(expr): if type == "null" then null else expr end;

###############################################################################
## helper functions for SQL generation
###############################################################################

# a handy way to quote SQL identifiers
# See: http://www.postgresql.org/docs/current/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
def asSqlIdent: "\"\( tostring  # anything can be turned into a SQL identifier if they are inside double quotes
                    | gsub("\""; "\"\"") # even double quote themselves (in fact, PostgreSQL says \0 cannot be included)
                    )\"";

# a handy way to turn things into SQL string literals (as it is hard to write single quote within single quote in this Bash script)
def asSqlLiteral:
    # map null to NULL
    if type == "null" then "NULL"
    # shouldn't turn primitive data types into string literals
    elif type | in({number:1, boolean:1}) then tostring
    # surround strings with single quotes with appropriate escaping
    # and turn any array or object into JSON format
    else "'\(tostring | gsub("'"; "''"))'"
    end;

# a handy way to generate a string from an array with prefix/suffix and delimiters
# where an empty string is desired for empty input
# to simplify the following SQL SELECT query generation
def mapJoinString(prefix; eachItem; delimiter; suffix):
    "\(prefix)\(map(eachItem) | join(delimiter))\(suffix)";
def mapJoinOrEmptyString(prefix; eachItem; delimiter; suffix):
    if type == null then null
    elif length == 0 then ""
    else
        if type == "array" then . else [.] end |
        mapJoinString(prefix; eachItem; delimiter; suffix)
    end;
# some short hands
def mapJoinString(prefix; eachItem; delimiter)        : mapJoinString(prefix; eachItem; delimiter; "");
def mapJoinString(prefix; eachItem)                   : mapJoinString(prefix; eachItem; "");
def mapJoinString(prefix)                             : mapJoinString(prefix; .);
def mapJoinOrEmptyString(prefix; eachItem; delimiter) : mapJoinOrEmptyString(prefix; eachItem; delimiter; "");
def mapJoinOrEmptyString(prefix; eachItem)            : mapJoinOrEmptyString(prefix; eachItem; "");
def mapJoinOrEmptyString(prefix)                      : mapJoinOrEmptyString(prefix; .);

# compile SQL expression: table.column or generic expression
def asSqlExpr:
    if has("expr")                       then .expr | tostring  # TODO support more structured expressions, e.g., binary operators and asSqlCondition
    elif has("table") and has("column") then "\(.table | asSqlIdent).\(.column | asSqlIdent)"
    elif has("column")                   then .column | asSqlIdent
    else error("Neither keys .expr or .column found for SQL expression in \(tostring)")
    end;
# compile `expression AS alias` in the field list of SELECT clauses
def asSqlExprAlias:
    (asSqlExpr) + (.alias | mapJoinOrEmptyString(" AS "; asSqlIdent));
# compile `table alias` mainly for the FROM or JOIN clauses
def asSqlTableAlias(asSql):
    if has("table") then
        (.table | asSqlIdent) +
        (.alias | mapJoinOrEmptyString(" "; asSqlIdent))
    elif has("sql") then
        "(\(.sql | asSql)) " +
        (.alias // error(".alias must be set for subquery \(.sql | tostring)") | asSqlIdent)
    else error("Neither keys .table or .sql found for FROM clause in \(tostring)")
    end;
def asSqlJoinTypeTableAlias(asSql):
    # five types of joins in ANSI SQL standard: https://en.wikipedia.org/wiki/Join_(SQL)
    (.LEFT_OUTER  | mapJoinOrEmptyString("LEFT OUTER JOIN "  ; asSqlTableAlias(asSql))) //
    (.RIGHT_OUTER | mapJoinOrEmptyString("RIGHT OUTER JOIN " ; asSqlTableAlias(asSql))) //
    (.FULL_OUTER  | mapJoinOrEmptyString("FULL OUTER JOIN "  ; asSqlTableAlias(asSql))) //
    (.INNER       | mapJoinOrEmptyString("INNER JOIN "       ; asSqlTableAlias(asSql))) //
    (.CROSS       | mapJoinOrEmptyString("CROSS JOIN "       ; asSqlTableAlias(asSql))) //
    error("Join table must be specified under one of these keys: LEFT_OUTER, RIGHT_OUTER, FULL_OUTER, INNER, CROSS, but found: \(tostring)");
# compile SQL conditional expressions for WHILE, HAVING, and JOIN ON clauses
# TODO maybe these comparisons should be folded into asSqlExpr
def asSqlCondition:
    def asSqlBinaryComparison(op): nullOr(
        if type == "array" and length == 2 then
            "\(.[0] | asSqlExpr) \(op) \(.[1] | asSqlExpr)"
        else error("Comparison '\(op)' expects exactly two expressions as an array, but found: \(tostring)")
        end);
    (.eq | asSqlBinaryComparison("=" )) //
    (.gt | asSqlBinaryComparison(">" )) //
    (.ge | asSqlBinaryComparison(">=")) //
    (.le | asSqlBinaryComparison("<=")) //
    (.lt | asSqlBinaryComparison("<" )) //
    (.isNull   | nullOr("\(asSqlExpr) IS NULL")) //
    (.isntNull | nullOr("\(asSqlExpr) IS NOT NULL")) //
    error("Unrecognized SQL condition \(tostring)");
# a more structured way to generate a SQL (Structured! Query Language) SELECT query than assembling strings
# which turns an object in a particular format into SQL, taking care of many escaping issues
def asSql:
    [ (.SELECT  |mapJoinOrEmptyString("SELECT "   ; asSqlExprAlias                                      ; "\n     , "))
    , (.FROM    |mapJoinOrEmptyString("FROM "     ; asSqlTableAlias(asSql)                              ; ", "))
    , (.JOIN    |mapJoinOrEmptyString(""; "\(asSqlJoinTypeTableAlias(asSql)) ON \(.ON | asSqlCondition)"; " " ))
    , (.WHERE   |mapJoinOrEmptyString("WHERE "    ; asSqlCondition                                      ; " AND " ))
    , (.GROUP_BY|mapJoinOrEmptyString("GROUP BY " ; asSqlExpr                                           ; ", "    ))
    , (.HAVING  |mapJoinOrEmptyString("HAVING "   ; asSqlCondition                                      ; " AND " ))
    , (.ORDER_BY|mapJoinOrEmptyString("ORDER BY " ; "\(.expr | asSqlExpr) \(.order // "ASC")"           ; ", "    ))
    ] | join("\n") | trimWhitespace;
## finally, a test case
#if
#    { SELECT:
#        [ { column: "id" }
#        , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
#        , { expr: "COALESCE(initvalue, 0)" }
#        ]
#    , FROM:
#        [ { table:"dd_weights_foo" }
#        ]
#    } | asSql | debug |
#false then . else . end |
###############################################################################

.deepdive_ as $deepdive
| "id" as $deepdiveVariableIdColumn

# TODO parse variable schema (mainly to find Multinomial cardinality)

# parse function and weight fields for every inference rules as function_ and weight_
| .deepdive_.inference.factors |= with_entries(
    (.key | ltrimstr("factor/")) as $factorName | .value |=

    # parse the weight field
    ( .weight_ = (.weight | trimWhitespace |
        if startswith("?")? then
            # unknown weight, find parameters
            { is_fixed: false
            , params: (ltrimstr("?") | trimWhitespace
                | ltrimstr("(") | rtrimstr(")") | trimWhitespace
                | if length == 0 then [] else  split("\\s*,\\s*") end)
            , init_value: 0.0
            }
        else
            # fixed weight
            { is_fixed: true
            , params: []
            , init_value: tonumber
            }
        end)

    # parse the function field
    | .function_ = (.function | trimWhitespace
        | capture("^ (?<name>.+)
                \\s* \\(
                \\s* (?<variables>.+)
                \\s* \\)
                   $"; "x") // error("\(.): Failed parsing function field for deepdive.inference.factors.\($factorName)")
        | .variables |= [ trimWhitespace | splits("\\s*,\\s*")
            # parse arguments to the function or predicate (variables)
            | capture("^ (?<isNegated>!)?
                    \\s* (?<relation>.+)
                    \\s* \\.
                    \\s* (?<field>[^.]+)
                    \\s* (?<isArray>\\[\\])?
                    \\s* (?: = (?<equalsTo> \\d+))?
                       $"; "x") // error("\(.): Failed parsing variable argument for function for deepdive.inference.factors.\($factorName)")
            | .isNegated |= (length > 0)
            | .isArray   |= (length > 0)
            | .equalsTo  |= (if . then tonumber else null end)
            ]
        | .id =
            # map function name to the code used by sampler (case insensitive)
            { imply       : 0
            , or          : 1
            , and         : 2
            , equal       : 3
            , istrue      : 4
            , multinomial : 5
            , linear      : 7
            , ratio       : 8
            , logical     : 9
            , imply3      : 11
            }[.name | ascii_downcase] //
                error("\(.name): deepdive.inference.factors.\($factorName) uses an unrecognized function")
        )
    )
)

| .deepdive_ as $deepdive  # necessary since we just mutated it


| .deepdive_.extraction.extractors += {

    # grounding the factor graph
    # TODO remove me
    "process/grounding/legacy": {
        dependencies_: ($deepdive.inference.factors | keys),
        output_: "model/factorgraph.legacy",
        style: "cmd_extractor",
        cmd: "mkdir -p ../../../model && cd ../../../model
            mkdir -p factorgraph

            set +x; . load-db-driver.sh; set -x
            export DEEPDIVE_LOGFILE=factorgraph/grounding.log
            [[ ! -e \"$DEEPDIVE_LOGFILE\" ]] || mv -f \"$DEEPDIVE_LOGFILE\" \"$DEEPDIVE_LOGFILE\"~
            java org.deepdive.Main -c <(
                set +x
                echo \("deepdive \(.deepdive | @json)" | @sh)
                echo \("deepdive.pipeline.pipelines.grounding: [\(.deepdive.inference.factors | keys | join(", "))]" | @sh)
                echo \("deepdive.pipeline.run: grounding" | @sh)
            ) -o factorgraph -t inference_grounding

            # drop graph. prefix from file names
            cd factorgraph
            mv -f graph.variables variables
            mv -f graph.factors   factors
            mv -f graph.weights   weights
            mv -f graph.meta      meta
        "
    },

    # first of all,
    # consecutive variable id range should be partitioned first by counting the variables
    "process/grounding/variable_id_partition": {
        dependencies_: [
            # id partition depends on all variable tables
            $deepdive.schema.variables | keys[] | "data/\(.)"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.schema.variables | keys | map(@sh) | join(" ")) | {
            # record the base
            variableCountTotal=0
            while read table begin excludeEnd; do
                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                echo $begin                      >id_begin
                echo $excludeEnd                 >id_exclude_end
                echo $(( $excludeEnd - $begin )) >count
                variableCountTotal=$excludeEnd
            done
            # record the final count
            echo $variableCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/variable_count
        }
        "
    },

    # each variable gets the consecutive ids assigned to its rows
    # (process/grounding/variable/*/assign_id)

    # app-wide holdout query is executed
    "process/grounding/variable_holdout": {
        dependencies_: [
            $deepdive.schema.variables | keys[] | "process/grounding/variable/\(.)/assign_id"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        for sql in \(
            [ " DROP TABLE IF EXISTS dd_graph_variables_holdout CASCADE;
              CREATE TABLE dd_graph_variables_holdout(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.holdout_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else
                # otherwise, randomly select from evidence variables of each variable table
                $deepdive.schema.variables | to_entries[]
                | .key as $relationName | .value | to_entries[] | .key as $columnName | "
                    INSERT INTO dd_graph_variables_holdout \(
                    { SELECT:
                        [ { column: $deepdiveVariableIdColumn }
                        ]
                    , FROM:
                        [ { table: $relationName }
                        ]
                    , WHERE:
                        [ { isntNull: { column: $columnName } }
                        , { lt: [ { expr: "RANDOM()" }
                                , { expr: $deepdive.calibration.holdout_fraction }
                                ]
                          }
                        ]
                    } | asSql);
                "
              end
            , " DROP TABLE IF EXISTS dd_graph_variables_observation CASCADE;
              CREATE TABLE dd_graph_variables_observation(variable_id BIGINT PRIMARY KEY);
              "
            , if $deepdive.calibration.observation_query then
                # run user holdout query if configured
                $deepdive.calibration.holdout_query
              else empty end
            ] | map(@sh) | join(" "))
        do deepdive sql \"$sql\"
        done
        "
    },

    # then variables are dumped into tsv
    # (See process/grounding/variable/*/dump below)

    # each inference rule input_query is run to materialize the factors and the distinct weights used in them
    # (See process/grounding/factor/*/materialize below)

    # in between the two steps for grounding all factors, weight id range must be decided serially
    "process/grounding/weight_id_partition": {
        dependencies_: [
            $deepdive.inference.factors | keys[]
            | "process/grounding/\(.)/materialize"
        ],
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

        # partition the id range for weights
        RANGE_BEGIN=0 \\
        partition_id_range \($deepdive.inference.factors | keys | map(
                ltrimstr("factor/") | "dd_weights_\(.)" | @sh) | join(" ")) | {
            weightsCountTotal=0
            while read table begin excludeEnd; do
                factor=${table#dd_weights_}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/${factor}
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                echo $begin                      >weights_id_begin
                echo $excludeEnd                 >weights_id_exclude_end
                echo $(( $excludeEnd - $begin )) >weights_count
                weightsCountTotal=$excludeEnd
            done
            echo $weightsCountTotal >\"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count
        }

        # set up a union view for all weight tables (dd_graph_weights)
        deepdive sql \("DROP TABLE IF EXISTS dd_graph_weights CASCADE;" | @sh) || true
        deepdive sql \("
            DROP VIEW IF EXISTS dd_graph_weights CASCADE;
            CREATE VIEW dd_graph_weights AS \(
                [ $deepdive.inference.factors | to_entries[] |
                    (.key | ltrimstr("factor/")) as $factorName | .value |
                    { SELECT:
                        [ { column: "id" }
                        , { column: "isfixed" }
                        , { column: "initvalue" }
                        , { alias: "description", expr:
                                [ ("\($factorName)-" | asSqlLiteral)
                                , (.weight_.params[] |
                                    "CASE WHEN \(asSqlIdent) IS NULL THEN ''
                                          ELSE \(asSqlIdent) || ''  -- XXX CAST(... AS TEXT) unsupported by MySQL
                                      END"
                                  )
                                ] | join(" ||\("-" | asSqlLiteral)|| ")
                          }
                        # TODO cardinality (actually class) column for multinomial as an offset to the id
                        ]
                    , FROM:
                        # TODO use exploded view for multinomial
                        [ { table: "dd_weights_\($factorName)" }
                        ]
                    } | asSql | "(\(.))"
                ] | join(" UNION ALL ")
            );" | @sh)
        "
    },

    # each inference rule gets weight ids actually assigned
    # (See process/grounding/factor/*/assign_weight_id below)

    # and the factors and weights are dumped into tsv
    # (See process/grounding/factor/*/dump below)

    # at the very end, everything grounded must be laid down in a format the sampler can load from
    "process/grounding/combine": {
        dependencies_: [(
            $deepdive.schema.variables | to_entries[]
            | .key as $relationName | .value | keys[] | . as $columnName
            | "process/grounding/variable/\($relationName)/dump"
        ), (
            $deepdive.inference.factors | keys[] | ltrimstr("factor/")
            | "process/grounding/factor/\(.)/dump"
        )],
        output_: "model/factorgraph",
        style: "cmd_extractor", cmd: "
        : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
        : ${DEEPDIVE_FACTORGRAPH_DIR:=\"$DEEPDIVE_APP\"/run/model/factorgraph}

        rm -rf   \"$DEEPDIVE_FACTORGRAPH_DIR\"
        mkdir -p \"$DEEPDIVE_FACTORGRAPH_DIR\"
        cd \"$DEEPDIVE_FACTORGRAPH_DIR\"

        # create symlinks to the grounded binaries by enumerating variables and factors
        for v in \([$deepdive.schema.variables | keys[] | @sh] | join(" ")); do
            mkdir -p variables/\"$v\"
            ( cd variables/\"$v\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/variable/\"$v\"/variables.part-*.bin . )
        done
        for f in \([$deepdive.inference.factors | keys[] | ltrimstr("factor/") | @sh] | join(" ")); do
            mkdir -p {factors,weights}/\"$f\"
            ( cd factors/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/factors.part-*.bin . )
            ( cd weights/\"$f\" && ln -sfn \"$DEEPDIVE_GROUNDING_DIR\"/factor/\"$f\"/weights.part-*.bin . )
        done

        # generate the metadata for the sampler
        {
            sumup() { { tr '\\n' +; echo 0; } | bc; }
            # first line with counts of variables and edges in the graph
            counts=()
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/weights_count))
            # sum up the number of factors and edges
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/variable_count))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nfactors.part-* | sumup))
            counts+=($(cat \"$DEEPDIVE_GROUNDING_DIR\"/factor/*/nedges.part-*   | sumup))
            IFS=,; echo \"${counts[*]}\"
            # second line with file paths
            paths=(\"$PWD\"/{weights,variables,factors,edges})
            IFS=,; echo \"${paths[*]}\"
        } >meta
        "
    }

}

# for each variable add some processes for grounding
| .deepdive_.extraction.extractors += merge($deepdive.schema.variables | to_entries[]
    | .key                   as $relationName
    | .value | to_entries[]
        | .key               as $columnName
        | .value             as $varType
        | "\($relationName)" as $varName
        | {

            # a process for assigning id to every variable according to the partition
            "process/grounding/variable/\($varName)/assign_id": {
                dependencies_: [
                    "process/grounding/variable_id_partition"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                cd \"$DEEPDIVE_GROUNDING_DIR\"/variable/${table}
                baseId=$(cat id_begin)

                # assign id to all rows according to the paritition
                db-assign_sequential_id $table \($deepdiveVariableIdColumn | @sh) $baseId

                # TODO generate_series multinomial classes
                "
            },

            # TODO easier way to do holdout per variable

            # a process for dumping each variable table
            "process/grounding/variable/\($varName)/dump": {
                dependencies_: [
                    "process/grounding/variable_holdout"
                  # XXX below can be omitted for now
                  #, "process/grounding/variable/\($varName)/assign_id"
                ],
                style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                table=\($relationName | @sh)
                #column=\($columnName | @sh)

                varPath=\"$DEEPDIVE_GROUNDING_DIR\"/variable/\($varName | @sh)
                mkdir -p \"$varPath\"
                cd \"$varPath\"
                rm -f variables-*.bin
                export DEEPDIVE_LOAD_FORMAT=tsv

                # dump the variables, joining the holdout query to determine the type of each variable
                deepdive compute \\
                    input_sql=\(
                    { SELECT:
                        [ { column: "id" }
                        , { column: "variable_role" }
                        , { alias: "init_value", expr:
                            "CASE WHEN variable_role = 0 THEN 0
                                  ELSE (CASE WHEN label THEN 1 ELSE 0 END) + 0.0
                              END" }
                              # TODO multinomial
                        , { column: "variable_type" }
                        , { column: "cardinality" }
                        ]
                    , FROM:
                        [ { alias: "variables", sql:
                            { SELECT:
                                [ { alias: "id", column: $deepdiveVariableIdColumn }
                                , { alias: "variable_role", expr:
                                      "CASE WHEN               observation.variable_id IS NOT NULL
                                             AND variables.\($columnName | asSqlIdent) IS NOT NULL THEN 2
                                            WHEN                   holdout.variable_id IS NOT NULL THEN 0
                                            WHEN variables.\($columnName | asSqlIdent) IS NOT NULL THEN 1
                                                                                                   ELSE 0
                                        END" }
                                , { alias: "label", table: "variables", column: $columnName }
                                , { alias: "variable_type", expr: (
                                        # TODO multinomial
                                        0  # for Boolean variables
                                    ) }
                                , { alias: "cardinality", expr: (
                                        # TODO multinomial
                                        2
                                    ) }
                                ]
                            , FROM:
                                [ { alias: "variables", table: $relationName }
                                ]
                            , JOIN:
                                [ { LEFT_OUTER:
                                    { alias: "holdout"
                                    , table: "dd_graph_variables_holdout"
                                    }
                                  , ON: { eq:
                                            [ { table: "variables", column: $deepdiveVariableIdColumn }
                                            , { table: "holdout"  , column: "variable_id" }
                                            ]
                                        }
                                  }
                                , { LEFT_OUTER:
                                    { alias: "observation"
                                    , table: "dd_graph_variables_observation"
                                    }
                                  , ON: { eq:
                                            [ { table: "variables"  , column: $deepdiveVariableIdColumn }
                                            , { table: "observation", column: "variable_id" }
                                            ]
                                        }
                                  }
                                ]
                            }
                          }
                        ]
                    } | asSql | @sh) \\
                    command=\("
                        format_converter variable /dev/stdin variables.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
                    " | @sh) \\
                    output_relation=
                "
            }

        })


# for each inference rule, add some processes for grounding the factors and weights
| .deepdive_.extraction.extractors += merge($deepdive.inference.factors | to_entries[]
    | (.key | ltrimstr("factor/")) as $factorName | .value
    | {
        # add a process for grounding factors
        "process/grounding/factor/\($factorName)/materialize": {
            # materializing each factor requires the dependent variables to have their id assigned
            dependencies_: [
                .input_[]
                | ltrimstr("data/") as $relationName
                | $deepdive.schema.variables[$relationName] | keys? | .[]
                | . as $columnName
                | "process/grounding/variable/\($relationName)/assign_id"
            ],
            # other non-variable tables are also necessary
            input_: [ .input_[]
                | select(ltrimstr("data/") | $deepdive.schema.variables[.] | not)
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                # materialize user input_query for the factor into a dd_query_* table
                deepdive sql \("dd_query_\($factorName)" as $factorTable | "
                    DROP TABLE IF EXISTS \($factorTable) CASCADE;
                    CREATE TABLE \($factorTable) AS
                        \(.input_query);
                " | @sh)

                # find distinct weights for the factor into a dd_weights_* table
                \(("dd_weights_\($factorName)" | asSqlIdent) as $weightsTable | "
                deepdive sql \("DROP VIEW IF EXISTS \($weightsTable) CASCADE;" | @sh) || true
                deepdive sql \("
                    DROP TABLE IF EXISTS \($weightsTable) CASCADE;
                    CREATE TABLE \($weightsTable) AS \(
                        { SELECT:
                            [ ( .weight_.params[] | { column: . } )
                            , { expr: .weight_.is_fixed  , alias: "isfixed" }
                            , { expr: .weight_.init_value, alias: "initvalue" }
                            , { expr: -1                 , alias: "id" }  # TODO cast to BIGINT?
                            ]
                        # when weight is parameterized, find all distinct ones
                        , FROM:
                            (if .weight_.params | length == 0 then [] else
                                [ { table: "dd_query_\($factorName)" }
                                ]
                            end)
                        , GROUP_BY:
                            (if .weight_.params | length == 0 then [] else
                                [ ( .weight_.params[] | { column: . } )
                                ]
                            end)
                        } | asSql);
                " | @sh)
                ")
            "
        },

        # add a process for grounding weights per inference rule
        "process/grounding/factor/\($factorName)/assign_weight_id": {
            dependencies_: [
                "process/grounding/weight_id_partition"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}

                cd \"$DEEPDIVE_GROUNDING_DIR\"/factor/\($factorName | @sh)
                baseId=$(cat weights_id_begin)

                # TODO consider step size = product of cardinalities for multinomial factors

                # assign weight ids according to the partition
                db-assign_sequential_id \("dd_weights_\($factorName)" | @sh) id $baseId
            "
        },

        # add a process for grounding factors and weights
        "process/grounding/factor/\($factorName)/dump": {
            dependencies_: [
                "process/grounding/factor/\($factorName)/assign_weight_id"
            ],
            style: "cmd_extractor", cmd: "
                : ${DEEPDIVE_GROUNDING_DIR:=\"$DEEPDIVE_APP\"/run/model/grounding}
                facPath=\"$DEEPDIVE_GROUNDING_DIR\"/factor/\($factorName | @sh)
                mkdir -p \"$facPath\"
                cd \"$facPath\"
                rm -f {factors,weights}.part-*.bin n{factors,edges}.part-*
                export DEEPDIVE_LOAD_FORMAT=tsv

                # dump the factors joining the assigned weight ids, converting into binary format for the sampler
                DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.factors\" \\
                deepdive compute \\
                    input_sql=\(
                        { SELECT:
                            [ { table: "weights", column: "id", alias: "weight_id" }
                            , ( .function_.variables[] |
                                { table: "factors", column: "\(.relation).\($deepdiveVariableIdColumn)" }
                              )
                            ]
                        , FROM:
                            [ { table: "dd_query_\($factorName)"  , alias: "factors" }
                            , { table: "dd_weights_\($factorName)", alias: "weights" }
                            ]
                        , WHERE:
                            [ .weight_.params[] |
                                { eq: [ { table: "factors", column: . }
                                      , { table: "weights", column: . }
                                      ]
                                }
                            # TODO multinomial should dump the weight_id for the first class
                            ]
                        } | asSql | @sh) \\
                    command=\("
                        # also record the factor count
                        tee >(wc -l >nfactors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}) |
                        format_converter factor /dev/stdin factors.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin \(.function_.id
                            ) \(.function_.variables | length
                            ) original \(.function_.variables | map(if .isNegated then "0" else "1" end) | join(" ")
                            ) |
                        # and the edge count
                        tee nedges.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}
                    " | @sh) \\
                    output_relation=

                # dump the weights (except the description column), converting into binary format for the sampler
                DEEPDIVE_CURRENT_PROCESS_NAME=\"${DEEPDIVE_CURRENT_PROCESS_NAME}.weights\" \\
                deepdive compute \\
                    input_sql=\(
                        { SELECT:
                            [ { column: "id" }
                            , { expr: "CASE WHEN isfixed THEN 1 ELSE 0 END" }
                            , { expr: "COALESCE(initvalue, 0)" }
                            ]
                        , FROM: [ { table:"dd_weights_\($factorName)" } ]
                        # TODO use exploded view with all combination of multinomial classes
                        } | asSql | @sh) \\
                    command=\("
                        format_converter weight /dev/stdin weights.part-${DEEPDIVE_CURRENT_PROCESS_INDEX}.bin
                    " | @sh) \\
                    output_relation=
            "
        }

    })

