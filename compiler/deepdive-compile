#!/usr/bin/env bash
# deepdive-compile -- Compiles DeepDive source code into executables under run/
##
# Author: Jaeho Shin <netj@cs.stanford.edu>
# Created: 2015-11-03
set -euo pipefail

DEEPDIVE_APP=$(find-deepdive-app)
export DEEPDIVE_APP
cd "$DEEPDIVE_APP"
. load-db-driver.sh

STEP() { echo >&2 "$@"; }

## compile a full deepdive.conf from source code
STEP "Compiling run/compiled.deepdive.conf and run/compiled.schema.json"
mkdir -p run
# compile schema in JSON
touch run/compiled.schema.json run/compiled.deepdive.conf
if [[ -r app.ddlog ]]; then
    ddlog export-schema app.ddlog
else
    echo '{}'
fi |
if [[ -r schema.json && -s schema.json ]]; then
    # merge user's schema.json
    SCHEMA_JSON="$(cat)" \
    jq '(env.SCHEMA_JSON | fromjson) + .' schema.json
else
    cat
fi >run/compiled.schema.json
{
# compile DDlog rules
if [[ -r app.ddlog ]]; then
    ddlog compile app.ddlog
fi
# append user's deepdive.conf
if [[ -r deepdive.conf ]]; then
    cat deepdive.conf
fi
# generate builtin extractors from schema
echo -n "deepdive.schema "
cat run/compiled.schema.json
} >run/compiled.deepdive.conf
cd run

# produce a JSON from compiled deepdive.conf
STEP "Converting HOCON to JSON as run/compiled.deepdive.conf.json"
(
# XXX some bogus variables in deepdive.conf added by DDlog compiler or in deepdive-default.conf
APP_HOME='"$DEEPDIVE_APP"'
PIPELINE=
PARALLELISM=1
INPUT_BATCH_SIZE=100000
export APP_HOME PIPELINE PARALLELISM INPUT_BATCH_SIZE
# convert HOCON into JSON
hocon2json \
    compiled.deepdive.conf \
    "$DEEPDIVE_HOME"/etc/deepdive-default.conf
) >compiled.deepdive.conf.json
STEP "Adding built-in processes in dataflow as normalized run/compiled.config.json"
{
# extend config with more builtin stuffs
configExtended |
# normalize config JSON
configNormalized |
# add builtin nodes
configCompleted
} <compiled.deepdive.conf.json >compiled.config.json

# validate/normalize the compiled.config.json
# TODO check if all input/output_relation exists in schema
# TODO check if all processes/pipelines depend on defined processes
# TODO check if all processes have required fields

STEP "Analyzing dependencies to generate run/Makefile and run/compiled.dataflow.svg"
# derive a dependency graph from the extractors, factors, pipelines defined in deepdive.conf
config2dependencies <compiled.config.json >compiled.dependencies.json
# compile a Makefile from the dependency graph
dependencies2make <compiled.dependencies.json >Makefile
# compile an SVG from the dependency graph
dependencies2dot <compiled.dependencies.json >compiled.dataflow.dot
dot -Tsvg <compiled.dataflow.dot >compiled.dataflow.svg

STEP "Compiling executable extractors and instructions for grounding factors"
# compile extractors and factors under process/ and factor/
config2compilation-plan <compiled.config.json >compiled.compilation-plan.sh
bash -eu ./compiled.compilation-plan.sh
