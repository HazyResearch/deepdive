#!/usr/bin/env bash
# deepdive-compile -- Compiles DeepDive source code into executables under run/
##
# Author: Jaeho Shin <netj@cs.stanford.edu>
# Created: 2015-11-03
set -euo pipefail

: ${DEEPDIVE_CONFIG_EXTRA:=}  # deepdive.conf can be augmented with extra content

DEEPDIVE_APP=$(find-deepdive-app)
export DEEPDIVE_APP
cd "$DEEPDIVE_APP"

. load-db-driver.sh
mkdir -p run

# log all the compilation steps and stderr with timestamps
exec 2> >(
    cd run
    [[ ! -e compile.log ]] || mv -f compile.log{,~}
    logging-with-ts ./compile.log >&2
)
STEP() { echo "$@"; } >&2

## compile a full deepdive.conf from source code
STEP "Compiling run/compiled.deepdive.conf and run/compiled.schema.json"
# compile schema in JSON
touch run/compiled.schema.json run/compiled.deepdive.conf
if [[ -r app.ddlog ]]; then
    ddlog export-schema app.ddlog
else
    echo '{}'
fi |
if [[ -r schema.json && -s schema.json ]]; then
    # merge user's schema.json
    SCHEMA_JSON="$(cat)" \
    jq '(env.SCHEMA_JSON | fromjson) + .' schema.json
else
    cat
fi >run/compiled.schema.json
{
# compile DDlog rules
if [[ -r app.ddlog ]]; then
    ddlog compile app.ddlog
fi
# append user's deepdive.conf
if [[ -r deepdive.conf ]]; then
    cat deepdive.conf
fi
# any extra config present in DEEPDIVE_CONFIG_EXTRA environment gets more priority
if [[ -n "$DEEPDIVE_CONFIG_EXTRA" ]]; then
    echo "$DEEPDIVE_CONFIG_EXTRA"
fi
# generate builtin extractors from schema
echo -n "deepdive.schema "
cat run/compiled.schema.json
} >run/compiled.deepdive.conf
cd run

# produce a JSON from compiled deepdive.conf
STEP "Converting into JSON as run/compiled.deepdive.conf.json"
(
# XXX some bogus variables in deepdive.conf added by DDlog compiler or in deepdive-default.conf
APP_HOME='"$DEEPDIVE_APP"'
PIPELINE=
PARALLELISM=1
INPUT_BATCH_SIZE=100000
export APP_HOME PIPELINE PARALLELISM INPUT_BATCH_SIZE
# convert HOCON into JSON
hocon2json \
    compiled.deepdive.conf \
    "$DEEPDIVE_HOME"/etc/deepdive-default.conf
) >compiled.deepdive.conf.json
STEP "Adding built-in processes in dataflow as normalized run/compiled.config.json"
{
# extend config with more builtin stuffs
compile-config_before_normalized |
# normalize config JSON
compile-config_normalized |
# add builtin nodes
compile-config_after_normalized
} <compiled.deepdive.conf.json >compiled.config.json

# validate/normalize the compiled.config.json
# TODO check if all input/output_relation exists in schema
# TODO check if all processes/pipelines depend on defined processes
# TODO check if all processes have required fields

STEP "Analyzing dependencies to generate run/Makefile and run/compiled.dataflow.svg"
# derive a dependency graph from the extractors, factors, pipelines defined in deepdive.conf
compile-config2dependencies <compiled.config.json >compiled.dependencies.json
# compile a Makefile from the dependency graph
compile-dependencies2make <compiled.dependencies.json >Makefile
# compile an SVG from the dependency graph
compile-dependencies2dot <compiled.dependencies.json >compiled.dataflow.dot
dot -Tsvg <compiled.dataflow.dot >compiled.dataflow.svg

STEP "Compiling executable code for extractors and factors as run/compiled.code.json"
# compile extractors and factors under process/ and factor/
# TODO A better idea would be to compile another JSON intermediate
# representation (IR) for codegen that can be almost directly mapped to the
# final shell scripts.
rm -f compiled.code-*.json
for cc in "$DEEPDIVE_HOME"/util/compile-code-*; do
    cc=${cc##*/}
    name=${cc#compile-code-}
    $cc compiled.config.json >compiled.code-$name.json
done
cat compiled.code-*.json |
jq -s . >compiled.code.json

STEP "Generating executable files under run/process/*"
compile-codegen <compiled.code.json
