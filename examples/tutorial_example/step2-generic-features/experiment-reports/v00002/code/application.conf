deepdive {
  
  db.default {
    driver: "org.postgresql.Driver"
    url: "jdbc:postgresql://"${PGHOST}":"${PGPORT}"/"${DBNAME} # "
    user: ${PGUSER}
    password: ${PGPASSWORD}
    dbname: ${DBNAME}
    host: ${PGHOST}
    port: ${PGPORT}
  }

  # Put your variables here
  schema.variables {
    has_spouse.is_true: Boolean
  }

  # Put your extractors here
  extraction.extractors {

    # Clean sentence table
    ext_clear_sentence {
      style: "sql_extractor"
      sql: """DELETE FROM sentences;"""
    }

    # Clean output tables of all extractors
    ext_clear_table {
      style: "sql_extractor"
      sql: """
        DELETE FROM people_mentions;
        DELETE FROM has_spouse;
        DELETE FROM has_spouse_features;
        """
    }

    # nlp_extractor only supports the default extractor.
    ext_sentences {
      input: """
        SELECT article_id, text 
        FROM articles 
        ORDER BY article_id ASC
        """
      output_relation: "sentences"
      udf: "examples/nlp_extractor/run.sh -k article_id -v text -l 120 -t 4"
      input_batch_size: 10
      output_batch_size: 1000
      parallelism: 8
      dependencies: ["ext_clear_sentence"]
    }

    # With a tsv_extractor, developers have to make sure arrays 
      # are parsable in the UDF. One easy way is to 
      # use "array_to_string(array, delimiter)" function by psql.
    ext_people {
      input: """
          SELECT  sentence_id, 
                  array_to_string(words, '~^~'), 
                  array_to_string(ner_tags, '~^~') 
          FROM    sentences
          """
      output_relation: "people_mentions"
      udf: ${APP_HOME}"/udf/ext_people.py"
      dependencies: ["ext_sentences", "ext_clear_table"]
      input_batch_size: 4000
      style: "tsv_extractor"
    }

    #     Table "public.people_mentions"
    #      Column     |  Type   | Modifiers
    # ----------------+---------+-----------
    #  sentence_id    | text    |
    #  start_position | integer |
    #  length         | integer |
    #  text           | text    |
    #  mention_id     | text    |
    ext_has_spouse_candidates {
      input: """
        SELECT  m.sentence_id,
                array_to_string(ARRAY_AGG(mention_id 
                  ORDER BY mention_id), '~^~') as mention_ids,
                array_to_string(ARRAY_AGG(text 
                  ORDER BY mention_id), '~^~') as texts,
                array_to_string(lemma, '~^~') as lemmas,
                array_to_string(ARRAY_AGG(start_position 
                  ORDER BY mention_id), '~^~') as start_positions,
                array_to_string(ARRAY_AGG(length 
                  ORDER BY mention_id), '~^~') as lengths
        FROM  people_mentions m, 
              sentences s
        WHERE m.sentence_id = s.sentence_id
        GROUP BY m.sentence_id, s.lemma;
      """
      output_relation: "has_spouse"
      udf: ${APP_HOME}"/udf/ext_has_spouse.py"
      dependencies: ["ext_people"]
      style: "tsv_extractor"
    }

    ext_has_spouse_features {
      input: """
        SELECT  array_to_string(words, '~^~'), 
                array_to_string(lemma, '~^~'),
                array_to_string(pos_tags, '~^~'),
                array_to_string(dependencies, '~^~'),
                array_to_string(ner_tags, '~^~'),
                has_spouse.relation_id, 
                p1.start_position, 
                p1.length, 
                p2.start_position, 
                p2.length
        FROM    has_spouse, 
                people_mentions p1, 
                people_mentions p2, 
                sentences
        WHERE   has_spouse.person1_id = p1.mention_id 
          AND   has_spouse.person2_id = p2.mention_id 
          AND   has_spouse.sentence_id = sentences.sentence_id;
        """
      output_relation: "has_spouse_features"
      udf: ${APP_HOME}"/udf/ext_has_spouse_features.py"
      dependencies: ["ext_has_spouse_candidates"]
      style: "tsv_extractor"
      parallelism: 4
    }

  }

  inference.factors: { 

    # We require developers to select: 
    #   - reserved "id" column, 
    #   - variable column, 
    #   - weight dependencies,
    # for variable tables.
    f_has_spouse_features {
      input_query: """
        SELECT  has_spouse.id AS "has_spouse.id", 
                has_spouse.is_true AS "has_spouse.is_true", 
                feature 
        FROM    has_spouse, 
                has_spouse_features 
        WHERE   has_spouse_features.relation_id = has_spouse.relation_id
        """
      function: "IsTrue(has_spouse.is_true)"
      weight: "?(feature)"
    }

    f_has_spouse_symmetry {
      input_query: """
        SELECT  r1.is_true AS "has_spouse.r1.is_true", 
                r2.is_true AS "has_spouse.r2.is_true", 
                r1.id AS "has_spouse.r1.id", 
                r2.id AS "has_spouse.r2.id"
        FROM    has_spouse r1, 
                has_spouse r2 
        WHERE   r1.person1_id = r2.person2_id 
          AND   r1.person2_id = r2.person1_id
          """
      function: "Equal(has_spouse.r1.is_true, has_spouse.r2.is_true)"
      # weight: "10" # We are pretty sure about this rule
      weight: "?" # should learn a positive weight
    }

    # Multiple spouse relations involving same mention in a sentence cannot be true at the same time.
    f_has_spouse_unique {
      input_query: """
        SELECT  r1.is_true AS "has_spouse.r1.is_true", 
                r2.is_true AS "has_spouse.r2.is_true", 
                r1.id AS "has_spouse.r1.id", 
                r2.id AS "has_spouse.r2.id"
        FROM    has_spouse r1, 
                has_spouse r2 
        WHERE   r1.person1_id = r2.person1_id 
          AND   r1.person2_id != r2.person2_id
          AND   r1.sentence_id = r2.sentence_id
          """
      function: "Equal(has_spouse.r1.is_true, has_spouse.r2.is_true)"
      weight: "?" # should learn a negative weight
    }

  }

  # # An example of how to use the last factor graph!
  # pipeline.relearn_from: ${DEEPDIVE_HOME}"/out/2014-12-20T184153/"

  # # If you want to re-extract all sentences:
  # pipeline.run: "nlp"
  # pipeline.pipelines.nlp: ["ext_sentences"]
  pipeline.run: "nonlp"
  pipeline.pipelines.nonlp: [
    "ext_people", 
    "ext_has_spouse_candidates", 
    "ext_has_spouse_features",
    "f_has_spouse_features",
    "f_has_spouse_symmetry",
    # "f_has_spouse_unique"
    ]

  # Specify a holdout fraction
  calibration.holdout_fraction: 0.25
  sampler.sampler_args: "-l 500 -s 1 -i 500 --alpha 0.1 --diminish 0.99"

}
