deepdive {

  db.default {
    driver: "org.postgresql.Driver"
    url: "jdbc:postgresql://"${PGHOST}":"${PGPORT}"/"${DBNAME} # "
    user: ${PGUSER}
    password: ${PGPASSWORD}
    dbname: ${DBNAME}
    host: ${PGHOST}
    port: ${PGPORT}
  }

  # Put your variables here
  schema.variables {
    has_spouse.is_true: Boolean
  }

  # Put your extractors here
  extraction.extractors {

    # Extractor 1: Clean output tables of all extractors
    ext_clear_table {
      style: "sql_extractor"
      sql: """
        DELETE FROM people_mentions;
        DELETE FROM has_spouse;
        DELETE FROM has_spouse_features;
        """
    }

    # Extractor 2: Extract mentions of people
    ext_people {

      # The style of the extractor
      style: "tsv_extractor"

      # An input to the extractor is a row (tuple) of the following query:
      input: """
          SELECT  sentence_id,
                  array_to_string(words, '~^~'),
                  array_to_string(ner_tags, '~^~')
          FROM    sentences
          """
      # With a tsv_extractor, developers have to make sure arrays
      # are parsable in the UDF. One easy way is to
      # use "array_to_string(array, delimiter)" function by psql.

      # output of extractor will be written to this table:
      output_relation: "people_mentions"

      # This user-defined function will be performed on each row (tuple) of input query:
      udf: ${APP_HOME}"/udf/ext_people.py"

      dependencies: ["ext_clear_table"]

    }

    # Extractor 3: Extract mention relation candidates
    ext_has_spouse_candidates {

      # The style of the extractor
      style: tsv_extractor

      # Each input (p1, p2) is a pair of mentions
      input: """
       SELECT p1.sentence_id,
              p1.mention_id, p1.text,
              p2.mention_id, p2.text
        FROM  people_mentions p1,
              people_mentions p2
        WHERE p1.sentence_id = p2.sentence_id
          AND p1.mention_id != p2.mention_id;
          """

      output_relation: "has_spouse"
      udf: ${APP_HOME}"/udf/ext_has_spouse.py"

      # Run this extractor after "ext_people"
      dependencies: ["ext_people"]

    }

    # Extractor 4: Extract features for relation candidates
    ext_has_spouse_features {
      style: "tsv_extractor"
      input: """
        SELECT  array_to_string(words, '~^~'),
                has_spouse.relation_id,
                p1.start_position,
                p1.length,
                p2.start_position,
                p2.length
        FROM    has_spouse,
                people_mentions p1,
                people_mentions p2,
                sentences
        WHERE   has_spouse.person1_id = p1.mention_id
          AND   has_spouse.person2_id = p2.mention_id
          AND   has_spouse.sentence_id = sentences.sentence_id;
        """

      output_relation : "has_spouse_features"
      udf             : ${APP_HOME}"/udf/ext_has_spouse_features.py"

      dependencies    : ["ext_has_spouse_candidates"]
    }

  }

# Put your inference rules here
  inference.factors: {

    # A simple logistic regression rule
    # We require developers to select:
    #   - reserved "id" column,
    #   - variable column,
    #   - weight dependencies,
    # for variable tables.
    f_has_spouse_features {

      # input to the inference rule is all the has_spouse candidate relations,
      #   as well as the features connected to them:
      input_query: """
        SELECT  has_spouse.id AS "has_spouse.id",
                has_spouse.is_true AS "has_spouse.is_true",
                feature
        FROM    has_spouse,
                has_spouse_features
        WHERE   has_spouse_features.relation_id = has_spouse.relation_id
        """

      # Factor function:
      function: "IsTrue(has_spouse.is_true)"

      # Weight of the factor is decided by the value of "feature" column in input query
      weight: "?(feature)"

    }

  }

  # # An example of how to use the last factor graph:
  # pipeline.relearn_from: ${DEEPDIVE_HOME}"/out/2014-12-22T153233/"

  # Default is to use the full pipeline, equivalent to:
  # pipeline.run: "all"
  # pipeline.pipelines.all: [
  #   "ext_people",
  #   "ext_has_spouse_candidates",
  #   "ext_has_spouse_features",
  #   "f_has_spouse_features"
  #   ]

  # Specify a holdout fraction to hold out randomly
  calibration.holdout_fraction: 0.25

  # A more scientific way is to hold out by sentence:
  calibration.holdout_query:"""
    DROP TABLE IF EXISTS holdout_sentence_ids CASCADE;

    CREATE TABLE holdout_sentence_ids AS
    SELECT sentence_id FROM sentences WHERE RANDOM() < 0.25;

    INSERT INTO dd_graph_variables_holdout(variable_id)
    SELECT id FROM has_spouse WHERE sentence_id IN
    (SELECT * FROM holdout_sentence_ids);
  """

  # You may also try tuning sampler arguments:
  # sampler.sampler_args: "-l 1000 -s 1 -i 1000 --alpha 0.1 --diminish 0.99"

}
